{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BM25 Hard Negative Selection + DPR Fine-Tuning",
      "provenance": [],
      "collapsed_sections": [
        "KLxyiIok9z83"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-KBPTsC-1qZ"
      },
      "source": [
        "# *Setup Python Environment and Libraries*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXUKKuT_IVX1"
      },
      "source": [
        "# Install BM25\n",
        "!pip install rank-bm25 nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12WBkLvQZh3R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cc73de2-0687-427d-a83c-dee7aa04f581"
      },
      "source": [
        "# Install the latest master of Haystack\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/deepset-ai/haystack.git\n",
            "  Cloning https://github.com/deepset-ai/haystack.git to /tmp/pip-req-build-5332jljx\n",
            "  Running command git clone -q https://github.com/deepset-ai/haystack.git /tmp/pip-req-build-5332jljx\n",
            "Collecting farm==0.8.0\n",
            "  Downloading farm-0.8.0-py3-none-any.whl (204 kB)\n",
            "\u001b[K     |████████████████████████████████| 204 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.68.1-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.15.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (1.1.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (0.0)\n",
            "Collecting elasticsearch<=7.10,>=7.7\n",
            "  Downloading elasticsearch-7.10.0-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 53.0 MB/s \n",
            "\u001b[?25hCollecting elastic-apm\n",
            "  Downloading elastic_apm-6.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 41.4 MB/s \n",
            "\u001b[?25hCollecting tox\n",
            "  Downloading tox-3.24.3-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (3.7.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 47.9 MB/s \n",
            "\u001b[?25hCollecting pytesseract==0.3.7\n",
            "  Downloading pytesseract-0.3.7.tar.gz (13 kB)\n",
            "Collecting pillow==8.3.2\n",
            "  Downloading Pillow-8.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 46.7 MB/s \n",
            "\u001b[?25hCollecting pdf2image==1.14.0\n",
            "  Downloading pdf2image-1.14.0-py3-none-any.whl (10 kB)\n",
            "Collecting sentence-transformers>=0.4.0\n",
            "  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (1.4.22)\n",
            "Collecting sqlalchemy_utils\n",
            "  Downloading SQLAlchemy_Utils-0.37.8-py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting faiss-cpu>=1.6.3\n",
            "  Downloading faiss_cpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 58.2 MB/s \n",
            "\u001b[?25hCollecting tika\n",
            "  Downloading tika-1.24.tar.gz (28 kB)\n",
            "Collecting httptools\n",
            "  Downloading httptools-0.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (3.2.5)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (8.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from farm-haystack==0.9.0) (2.6.2)\n",
            "Collecting pymilvus\n",
            "  Downloading pymilvus-1.1.2-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting SPARQLWrapper\n",
            "  Downloading SPARQLWrapper-1.8.5-py3-none-any.whl (26 kB)\n",
            "Collecting mmh3\n",
            "  Downloading mmh3-3.0.0-cp37-cp37m-manylinux2010_x86_64.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting weaviate-client==2.5.0\n",
            "  Downloading weaviate_client-2.5.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting ray==1.5.0\n",
            "  Downloading ray-1.5.0-cp37-cp37m-manylinux2014_x86_64.whl (51.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 51.5 MB 24 kB/s \n",
            "\u001b[?25hCollecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 53.6 MB/s \n",
            "\u001b[?25hCollecting uvloop==0.14\n",
            "  Downloading uvloop-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (1.4.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.0 MB/s \n",
            "\u001b[?25hCollecting transformers==4.6.1\n",
            "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (5.4.8)\n",
            "Collecting Werkzeug==0.16.1\n",
            "  Downloading Werkzeug-0.16.1-py2.py3-none-any.whl (327 kB)\n",
            "\u001b[K     |████████████████████████████████| 327 kB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (0.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (57.4.0)\n",
            "Collecting dotmap\n",
            "  Downloading dotmap-1.3.24-py3-none-any.whl (11 kB)\n",
            "Collecting torch<1.9,>1.5\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.4 kB/s \n",
            "\u001b[?25hCollecting flask-restplus\n",
            "  Downloading flask_restplus-0.13.0-py2.py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 28.9 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (0.37.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.18.41-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 69.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (4.62.0)\n",
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from farm==0.8.0->farm-haystack==0.9.0) (1.1.4)\n",
            "Collecting mlflow<=1.13.1\n",
            "  Downloading mlflow-1.13.1-py3-none-any.whl (14.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.1 MB 27 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray==1.5.0->farm-haystack==0.9.0) (1.19.5)\n",
            "Collecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray==1.5.0->farm-haystack==0.9.0) (3.0.12)\n",
            "Collecting pydantic>=1.8\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 45.0 MB/s \n",
            "\u001b[?25hCollecting opencensus\n",
            "  Downloading opencensus-0.7.13-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 56.6 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 47.2 MB/s \n",
            "\u001b[?25hCollecting redis>=3.5.0\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 474 kB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting aioredis\n",
            "  Downloading aioredis-2.0.0-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray==1.5.0->farm-haystack==0.9.0) (1.39.0)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray==1.5.0->farm-haystack==0.9.0) (0.11.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray==1.5.0->farm-haystack==0.9.0) (1.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray==1.5.0->farm-haystack==0.9.0) (3.13)\n",
            "Collecting gpustat\n",
            "  Downloading gpustat-0.6.0.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray==1.5.0->farm-haystack==0.9.0) (2.6.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray==1.5.0->farm-haystack==0.9.0) (7.1.2)\n",
            "Collecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.3.9-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray==1.5.0->farm-haystack==0.9.0) (3.17.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1->farm==0.8.0->farm-haystack==0.9.0) (4.6.4)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 72.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1->farm==0.8.0->farm-haystack==0.9.0) (21.0)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.1->farm==0.8.0->farm-haystack==0.9.0) (2019.12.20)\n",
            "Collecting validators>=0.18.2\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==0.9.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray==1.5.0->farm-haystack==0.9.0) (1.15.0)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (0.4.1)\n",
            "Collecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (0.3)\n",
            "Collecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 59.3 MB/s \n",
            "\u001b[?25hCollecting alembic<=1.4.1\n",
            "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (2.8.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (1.3.0)\n",
            "Collecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.15.0.tar.gz (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting azure-storage-blob>=12.0.0\n",
            "  Downloading azure_storage_blob-12.8.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[K     |████████████████████████████████| 345 kB 57.0 MB/s \n",
            "\u001b[?25hCollecting docker>=4.0.0\n",
            "  Downloading docker-5.0.2-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 61.3 MB/s \n",
            "\u001b[?25hCollecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.18.2.tar.gz (22 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting azure-core<2.0.0,>=1.10.0\n",
            "  Downloading azure_core-1.18.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 77.0 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.1.4\n",
            "  Downloading cryptography-3.4.8-cp36-abi3-manylinux_2_24_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 42.0 MB/s \n",
            "\u001b[?25hCollecting msrest>=0.6.18\n",
            "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (2.20)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (0.8.9)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.18->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.8.0->farm-haystack==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm==0.8.0->farm-haystack==0.9.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm==0.8.0->farm-haystack==0.9.0) (3.1.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.0->farm-haystack==0.9.0) (0.10.0+cu102)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.0->farm-haystack==0.9.0) (0.22.2.post1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.4.2->farm-haystack==0.9.0) (1.1.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators>=0.18.2->weaviate-client==2.5.0->farm-haystack==0.9.0) (4.4.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray==1.5.0->farm-haystack==0.9.0) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 60.5 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 61.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.41\n",
            "  Downloading botocore-1.21.41-py3-none-any.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 73.6 MB/s \n",
            "\u001b[?25hCollecting urllib3<2,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting starlette==0.14.2\n",
            "  Downloading starlette-0.14.2-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.8.0->farm-haystack==0.9.0) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->farm==0.8.0->farm-haystack==0.9.0) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->farm==0.8.0->farm-haystack==0.9.0) (2.0.1)\n",
            "Collecting aniso8601>=0.82\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask-restplus->farm==0.8.0->farm-haystack==0.9.0) (2018.9)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray==1.5.0->farm-haystack==0.9.0) (7.352.0)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.6.1->farm==0.8.0->farm-haystack==0.9.0) (3.5.0)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray==1.5.0->farm-haystack==0.9.0) (1.26.3)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.5.0->farm-haystack==0.9.0) (1.34.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.5.0->farm-haystack==0.9.0) (1.53.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.5.0->farm-haystack==0.9.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.5.0->farm-haystack==0.9.0) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.5.0->farm-haystack==0.9.0) (4.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.1->farm==0.8.0->farm-haystack==0.9.0) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.5.0->farm-haystack==0.9.0) (0.4.8)\n",
            "Collecting ujson>=2.0.0\n",
            "  Downloading ujson-4.1.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 51.8 MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.28.1\n",
            "  Downloading grpcio-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 37.9 MB/s \n",
            "\u001b[?25hCollecting grpcio-tools<1.38.0,>=1.22.0\n",
            "  Downloading grpcio_tools-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx->farm-haystack==0.9.0) (4.2.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.1->farm==0.8.0->farm-haystack==0.9.0) (1.0.1)\n",
            "Collecting rdflib>=4.0\n",
            "  Downloading rdflib-6.0.0-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 61.5 MB/s \n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.6 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 121 kB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.7/dist-packages (from tox->farm-haystack==0.9.0) (0.10.2)\n",
            "Collecting pluggy>=0.12.0\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: py>=1.4.17 in /usr/local/lib/python3.7/dist-packages (from tox->farm-haystack==0.9.0) (1.10.0)\n",
            "Collecting virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0\n",
            "  Downloading virtualenv-20.7.2-py2.py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 45.7 MB/s \n",
            "\u001b[?25hCollecting backports.entry-points-selectable>=1.0.4\n",
            "  Downloading backports.entry_points_selectable-1.1.0-py2.py3-none-any.whl (6.2 kB)\n",
            "Collecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.3.0-py3-none-any.whl (13 kB)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.2-py2.py3-none-any.whl (338 kB)\n",
            "\u001b[K     |████████████████████████████████| 338 kB 60.2 MB/s \n",
            "\u001b[?25hCollecting h11>=0.8\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting asgiref>=3.4.0\n",
            "  Downloading asgiref-3.4.1-py3-none-any.whl (25 kB)\n",
            "Building wheels for collected packages: farm-haystack, pytesseract, alembic, databricks-cli, sentence-transformers, gpustat, langdetect, prometheus-flask-exporter, python-docx, python-multipart, seqeval, tika\n",
            "  Building wheel for farm-haystack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm-haystack: filename=farm_haystack-0.9.0-py3-none-any.whl size=199417 sha256=f8d50ae32c52c5404c00454c8cfdb0a52371cff0a95568f19326fb31ec8bca5f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w7k66us3/wheels/a7/05/3b/9b33368d9af06a39f8e6af2e97fa2af876e893ade323cfc2c9\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.7-py2.py3-none-any.whl size=13954 sha256=b42421bc71eb12f679c01741164bd4a02197e1ac2df1f74bb361e3dfe7144783\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/71/6c/7a8c5ca2e699752506999ae7baeb692e2b4fc6488c2cddcb22\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158172 sha256=cd925f41bf5979e47b5a68ad6662ea2316e8acea8c220300d956ab080925713c\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.15.0-py3-none-any.whl size=105260 sha256=2641765f4ef8ff03bfe5a71aaf871246a3c3b669fa1f0f9ab9313062a459cd0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/ba/75/284f9a90ff7a010bb23b9798f2e9a19dd9fe619379c917bff4\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-py3-none-any.whl size=126710 sha256=cc6e79ef432dbd7637052b80a3bf251333c3d31b50114b091f67c76ebda0f3c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c1/0f/faafd427f705c4b012274ba60d9a91d75830306811e1355293\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=1d194e493cec0533960bfe1db2504f0091217beecfc4e52902276352dfc7fe74\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/67/af/f1ad15974b8fd95f59a63dbf854483ebe5c7a46a93930798b8\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=641d036a988ddd0b1f05b550e4725ca9dd7fb390f2b782fd06ad872bb744f9cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-py3-none-any.whl size=17415 sha256=6764db0a8c10809f8613b971db6db4d155ad7b9df78e6f85a49ae2a378030d13\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/1e/1c/c765920cb92b2f0343d2dd8b481a407cee2823f9b4bbd2e52a\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=cd4fb1bd8c1ac51d7030f0f86cf25d0cc029a34a79d1b34cdceb2ba047c279f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=c2c6250b15d6452fee61c115c348dbc33ae875f1795db26da6db11377933dfcd\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=55cd33496f0b467f48b4b0a54fc5a9abd0456b10024470069261f3322ce02be9\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32891 sha256=c3df37e5748c17619b3a9b66e0366d707ef1b53ca30e6f7e5bde358bf7d7f7ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
            "Successfully built farm-haystack pytesseract alembic databricks-cli sentence-transformers gpustat langdetect prometheus-flask-exporter python-docx python-multipart seqeval tika\n",
            "Installing collected packages: urllib3, Werkzeug, smmap, multidict, jmespath, isodate, yarl, websocket-client, python-editor, msrest, Mako, gitdb, cryptography, botocore, azure-core, async-timeout, torch, tokenizers, sacremoses, s3transfer, querystring-parser, prometheus-flask-exporter, platformdirs, pillow, opencensus-context, huggingface-hub, gunicorn, grpcio, gitpython, docker, distlib, databricks-cli, blessings, backports.entry-points-selectable, azure-storage-blob, aniso8601, alembic, aiohttp, virtualenv, validators, ujson, transformers, torchvision, starlette, seqeval, sentencepiece, redis, rdflib, pydantic, py-spy, pluggy, opencensus, mlflow, h11, grpcio-tools, gpustat, flask-restplus, flask-cors, dotmap, colorama, boto3, asgiref, aioredis, aiohttp-cors, weaviate-client, uvloop, uvicorn, tox, tika, sqlalchemy-utils, SPARQLWrapper, sentence-transformers, ray, python-multipart, python-docx, pytesseract, pymilvus, psycopg2-binary, pdf2image, mmh3, langdetect, httptools, fastapi, farm, faiss-cpu, elasticsearch, elastic-apm, farm-haystack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.39.0\n",
            "    Uninstalling grpcio-1.39.0:\n",
            "      Successfully uninstalled grpcio-1.39.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\n",
            "pytest 3.6.4 requires pluggy<0.8,>=0.5, but you have pluggy 1.0.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Mako-1.1.5 SPARQLWrapper-1.8.5 Werkzeug-0.16.1 aiohttp-3.7.4.post0 aiohttp-cors-0.7.0 aioredis-2.0.0 alembic-1.4.1 aniso8601-9.0.1 asgiref-3.4.1 async-timeout-3.0.1 azure-core-1.18.0 azure-storage-blob-12.8.1 backports.entry-points-selectable-1.1.0 blessings-1.7 boto3-1.18.41 botocore-1.21.41 colorama-0.4.4 cryptography-3.4.8 databricks-cli-0.15.0 distlib-0.3.2 docker-5.0.2 dotmap-1.3.24 elastic-apm-6.4.0 elasticsearch-7.10.0 faiss-cpu-1.7.1.post2 farm-0.8.0 farm-haystack-0.9.0 fastapi-0.68.1 flask-cors-3.0.10 flask-restplus-0.13.0 gitdb-4.0.7 gitpython-3.1.18 gpustat-0.6.0 grpcio-1.37.1 grpcio-tools-1.37.1 gunicorn-20.1.0 h11-0.12.0 httptools-0.3.0 huggingface-hub-0.0.8 isodate-0.6.0 jmespath-0.10.0 langdetect-1.0.9 mlflow-1.13.1 mmh3-3.0.0 msrest-0.6.21 multidict-5.1.0 opencensus-0.7.13 opencensus-context-0.1.2 pdf2image-1.14.0 pillow-8.3.2 platformdirs-2.3.0 pluggy-1.0.0 prometheus-flask-exporter-0.18.2 psycopg2-binary-2.9.1 py-spy-0.3.9 pydantic-1.8.2 pymilvus-1.1.2 pytesseract-0.3.7 python-docx-0.8.11 python-editor-1.0.4 python-multipart-0.0.5 querystring-parser-1.2.4 ray-1.5.0 rdflib-6.0.0 redis-3.5.3 s3transfer-0.5.0 sacremoses-0.0.45 sentence-transformers-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 smmap-4.0.0 sqlalchemy-utils-0.37.8 starlette-0.14.2 tika-1.24 tokenizers-0.10.3 torch-1.8.1 torchvision-0.9.1 tox-3.24.3 transformers-4.6.1 ujson-4.1.0 urllib3-1.25.11 uvicorn-0.15.0 uvloop-0.14.0 validators-0.18.2 virtualenv-20.7.2 weaviate-client-2.5.0 websocket-client-1.2.1 yarl-1.6.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzJ945wJQOAu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "ac2764b2-3bd6-46ca-ca22-ecab075dc45b"
      },
      "source": [
        "# Import libraries \n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path, PurePath\n",
        "import requests\n",
        "from requests.exceptions import HTTPError, ConnectionError\n",
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import math \n",
        "from google.colab import files\n",
        "\n",
        "# Import DPR stuff\n",
        "from haystack.retriever.dense import DensePassageRetriever\n",
        "from haystack.preprocessor.utils import fetch_archive_from_http\n",
        "from haystack.document_store.memory import InMemoryDocumentStore\n",
        "from haystack.generator.transformers import RAGenerator\n",
        "from haystack.document_store.faiss import FAISSDocumentStore"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "  \"update your install command.\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f3626f08cb70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Import DPR stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDensePassageRetriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_archive_from_http\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryDocumentStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haystack/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseComponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haystack/pipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseComponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseDocumentStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haystack/generator/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haystack/generator/transformers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfarm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minitialize_device_settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRagTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRagTokenForGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/farm/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVERSION\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_configure_mlflow_loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfluent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfluent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlflow/tracking/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMlflowClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from mlflow.tracking._tracking_service.utils import (\n\u001b[1;32m     10\u001b[0m     \u001b[0mset_tracking_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_registry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDEFAULT_AWAIT_MAX_SLEEP_SECONDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracking_service\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracking_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrackingServiceClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_upload_artifacts_to_databricks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnsupportedModelRegistryStoreURIException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlflow/tracking/_tracking_service/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunStatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunTag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mViewType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExperimentTag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact_repository_registry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_artifact_repository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlflow_tags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLFLOW_USER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_string_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlflow/store/artifact/artifact_repository_registry.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMlflowException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mazure_blob_artifact_repo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAzureBlobArtifactRepository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbfs_artifact_repo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdbfs_artifact_repo_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp_artifact_repo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFTPArtifactRepository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcs_artifact_repo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCSArtifactRepository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlflow/store/artifact/dbfs_artifact_repo.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrest_store\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRestStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact_repo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArtifactRepository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabricks_artifact_repo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatabricksArtifactRepository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_artifact_repo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLocalArtifactRepository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracking_service\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mlflow/store/artifact/databricks_artifact_repo.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientAuthenticationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlobClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/azure/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pipeline_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_match_conditions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatchConditions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_enum_meta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCaseInsensitiveEnumMeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/azure/core/_pipeline_client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfiguration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineClientBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m from .pipeline.policies import (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/azure/core/pipeline/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m  \u001b[0;31m# pylint: disable=wrong-import-position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PipelineRequest\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PipelineResponse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PipelineContext\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/azure/core/pipeline/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mPipelineContext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTTPPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSansIOHTTPPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mawait_result\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_await_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/azure/core/pipeline/policies/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_redirect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRedirectPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetryPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRetryMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_distributed_tracing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedTracingPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m from ._universal import (\n\u001b[1;32m     34\u001b[0m     \u001b[0mHeadersPolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LvNKDZE--Vx"
      },
      "source": [
        "# BM25 Hard Negative Selection\n",
        "\n",
        "The following section cleans the synthetic data and finds hard negatives for each synthetic QA pair to use for DPR fine-tuning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6vz9ikxVckN"
      },
      "source": [
        "# Setup functions for cleaning text and stop word dropping  \n",
        "english_stopwords = list(set(stopwords.words('english')))\n",
        "\n",
        "def strip_characters(text):\n",
        "    t = re.sub('\\(|\\)|:|,|;|\\.|’|”|“|\\?|%|>|<', '', text)\n",
        "    t = re.sub('/', ' ', t)\n",
        "    t = t.replace(\"'\",'')\n",
        "    return t\n",
        "\n",
        "def clean(text):\n",
        "    t = text.lower()\n",
        "    t = strip_characters(t)\n",
        "    return t\n",
        "\n",
        "def tokenize(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return list(set([word for word in words \n",
        "                            if len(word) > 1\n",
        "                            and not word in english_stopwords\n",
        "                            and not word.isnumeric()\n",
        "                            and word.isalpha()\n",
        "                    ]\n",
        "                   )\n",
        "                )\n",
        "\n",
        "def preprocess(text):\n",
        "    t = clean(text)\n",
        "    tokens = tokenize(t)\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTsS3RkQGoEC"
      },
      "source": [
        "The following two chunks downloads the required data to be processed for selecting hard negatives. \n",
        "\n",
        "Both are essentially grabbing a google drive link that downloads the following csv:\n",
        "\n",
        "1. QA.csv (synthetic QA generated by Shamane)\n",
        "2. covid_data_full.csv (CORD19 data processed from the BM25 notebook) <- Currently not used because still testing pipeline, so rest of the code uses QA.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdwV6S9gdQ2f"
      },
      "source": [
        "# Download QA.csv (generated by Shamane)\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1kKZSBpgDwRCvaMR9y9caEs1wSdbFKRzs' -O QA.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-omxpdRceVqs"
      },
      "source": [
        "# Download Covid data full\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1zjw7U1bufzIU1j8HaW7NvkGNn9myYiDb' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1zjw7U1bufzIU1j8HaW7NvkGNn9myYiDb\" -O covid_data_full.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIfbR_jC9xOa"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ZeOqiN4duXO0IO_TMQHpiAar3AJUhziR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ZeOqiN4duXO0IO_TMQHpiAar3AJUhziR\" -O covid_dump.csv && rm -rf /tmp/cookies.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30LJWtoPQSyy"
      },
      "source": [
        "# NOT CURRENTLY USED \n",
        "# corona_df = pd.read_csv(\"covid_data_full.csv\")\n",
        "# print(corona_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh-tgSOZe4hk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6239097-5df1-41c0-ae3b-5017a1af9b1a"
      },
      "source": [
        "# Create hard negatives using BM25 using this data\n",
        "qa = pd.read_csv(\"QA.csv\")\n",
        "print(qa.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                                          question  ...                                                                            title\n",
            "0       What is an example of a virus that initiates sg mRNA synthesis internally?  ...  Sequence requirements for RNA strand transfer during nidovirus discontinuous...\n",
            "1  What has been suggested to explain the cotranscriptional fusion of noncontig...  ...  Sequence requirements for RNA strand transfer during nidovirus discontinuous...\n",
            "2          What is not signi®cantly affected by leader TRS and body TRS mutations?  ...  Sequence requirements for RNA strand transfer during nidovirus discontinuous...\n",
            "3                         What has the crucial role in nidovirus sg RNA synthesis?  ...  Sequence requirements for RNA strand transfer during nidovirus discontinuous...\n",
            "4  What does the single leader TRS mutation at all six Nidovirus discontinuous ...  ...  Sequence requirements for RNA strand transfer during nidovirus discontinuous...\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmbIhtTEU4la"
      },
      "source": [
        "The above sets up the python environment and loads in the data.\n",
        "covid_data_full.csv is the data from the BM25 notebook processed i.e. it is the AllenAI CORD19 data processed. \n",
        "\n",
        "The QA.csv is the data from Shamane.\n",
        "\n",
        "Currently, the covid_data_full.csv is too large to process fully so start with QA.csv\n",
        "\n",
        "Approach for creating hard negatives \n",
        "\n",
        "1. Use BM25 to get a list of passages from synthetic question. The corpus of passages should not include the context for that synthetic question. We should get top-k passages that doesn't contain the answer? This is a bit naive so we can do more ...  \n",
        "    1. For each of the top-k passages selected using BM25 we want to generate an answer\n",
        "    2. Do similarity to synthetic answer \n",
        "    3. Further subset the top-k passages based on the lowest similarity score \n",
        "2. Now we have, for each synthetic QA pair, we have: $\\{q_i,a_i|p_i^+,p_{i,m}^-\\}$ where m is the number of hard negative passages selected via the BM25\n",
        "\n",
        "DPR format\n",
        "\n",
        "```python\n",
        "[\n",
        "    {\n",
        "        \"question\": \"....\",\n",
        "        \"answers\": [\"...\", \"...\", \"...\"],\n",
        "        \"positive_ctxs\": [{\n",
        "            \"title\": \"...\",\n",
        "            \"text\": \"....\"\n",
        "        }],\n",
        "        \"negative_ctxs\": [\"...\"],\n",
        "        \"hard_negative_ctxs\": [\"...\"]\n",
        "    },\n",
        "    ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcpstU2tia01"
      },
      "source": [
        "# Process the qa.csv file and create a BM25 corpus \n",
        "BM25Corpus = qa.context.fillna(\"\").apply(preprocess).to_frame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm_0EeLxs58B"
      },
      "source": [
        "#Knowledge Base with Cord-19 data\n",
        "covid_dump = pd.read_csv(\"covid_dump.csv\",sep='\\t',header=0, names=['title','context'])\n",
        "BM25Corpus = covid_dump.context.fillna(\"\").apply(preprocess).to_frame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qskj1HJLibxF"
      },
      "source": [
        " # Create the BM25 object\n",
        " BM25 = BM25Okapi(BM25Corpus.context.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXH4k250ittU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "1b73f634-af6f-4ee0-fb82-b5472bc8e294"
      },
      "source": [
        "qa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>context</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is an example of a virus that initiates sg mRNA synthesis internally?</td>\n",
              "      <td>brome mosaic virus</td>\n",
              "      <td>Some viruses, such as brome mosaic virus, initiate sg mRNA synthesis interna...</td>\n",
              "      <td>Sequence requirements for RNA strand transfer during nidovirus discontinuous...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What has been suggested to explain the cotranscriptional fusion of noncontig...</td>\n",
              "      <td>Various models</td>\n",
              "      <td>Various models have been put forward to explain the cotranscriptional fusion...</td>\n",
              "      <td>Sequence requirements for RNA strand transfer during nidovirus discontinuous...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is not signi®cantly affected by leader TRS and body TRS mutations?</td>\n",
              "      <td>EAV genome replication</td>\n",
              "      <td>EAV genome replication is not signi®cantly affected by leader TRS and body T...</td>\n",
              "      <td>Sequence requirements for RNA strand transfer during nidovirus discontinuous...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What has the crucial role in nidovirus sg RNA synthesis?</td>\n",
              "      <td>base pairing between the sense leader TRS and the antisense body TRS</td>\n",
              "      <td>Recently, we have established the pivotal role of an interaction between sen...</td>\n",
              "      <td>Sequence requirements for RNA strand transfer during nidovirus discontinuous...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What does the single leader TRS mutation at all six Nidovirus discontinuous ...</td>\n",
              "      <td>RNA7 synthesis</td>\n",
              "      <td>In contrast to our ®ndings with the body TRS mutants, we did not obtain lead...</td>\n",
              "      <td>Sequence requirements for RNA strand transfer during nidovirus discontinuous...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87326</th>\n",
              "      <td>How many isolates had the gene encoding OXA-23 carbapenemase?</td>\n",
              "      <td>Twenty-six</td>\n",
              "      <td>No. of COVID-19 admissions met multidrug-resistant CRAB criteria. Thirty iso...</td>\n",
              "      <td>Increase in Hospital-Acquired Carbapenem-Resistant Acinetobacter baumannii I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87327</th>\n",
              "      <td>What did environmental services disinfect with bleach?</td>\n",
              "      <td>common areas and high-touch surfaces of ICUs</td>\n",
              "      <td>In early May, hospital A's IPC leadership advised physicians, unit managers,...</td>\n",
              "      <td>Increase in Hospital-Acquired Carbapenem-Resistant Acinetobacter baumannii I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87328</th>\n",
              "      <td>What did NJDOH investigate?</td>\n",
              "      <td>the cluster</td>\n",
              "      <td>In collaboration with hospital A, NJDOH investigated the cluster, including ...</td>\n",
              "      <td>Increase in Hospital-Acquired Carbapenem-Resistant Acinetobacter baumannii I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87329</th>\n",
              "      <td>What led to deviations in IPC practices?</td>\n",
              "      <td>Strategies to preserve continuity of care</td>\n",
              "      <td>A New Jersey hospital reported a cluster of 34 CRAB cases that peaked during...</td>\n",
              "      <td>Increase in Hospital-Acquired Carbapenem-Resistant Acinetobacter baumannii I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87330</th>\n",
              "      <td>What could hospitals with COVID-19 be vulnerable to?</td>\n",
              "      <td>outbreaks of multidrug-resistant organism infections</td>\n",
              "      <td>Hospitals managing surges of patients with COVID-19 might be vulnerable to o...</td>\n",
              "      <td>Increase in Hospital-Acquired Carbapenem-Resistant Acinetobacter baumannii I...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>87331 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                              question  ...                                                                            title\n",
              "0           What is an example of a virus that initiates sg mRNA synthesis internally?  ...  Sequence requirements for RNA strand transfer during nidovirus discontinuous...\n",
              "1      What has been suggested to explain the cotranscriptional fusion of noncontig...  ...  Sequence requirements for RNA strand transfer during nidovirus discontinuous...\n",
              "2              What is not signi®cantly affected by leader TRS and body TRS mutations?  ...  Sequence requirements for RNA strand transfer during nidovirus discontinuous...\n",
              "3                             What has the crucial role in nidovirus sg RNA synthesis?  ...  Sequence requirements for RNA strand transfer during nidovirus discontinuous...\n",
              "4      What does the single leader TRS mutation at all six Nidovirus discontinuous ...  ...  Sequence requirements for RNA strand transfer during nidovirus discontinuous...\n",
              "...                                                                                ...  ...                                                                              ...\n",
              "87326                    How many isolates had the gene encoding OXA-23 carbapenemase?  ...  Increase in Hospital-Acquired Carbapenem-Resistant Acinetobacter baumannii I...\n",
              "87327                           What did environmental services disinfect with bleach?  ...  Increase in Hospital-Acquired Carbapenem-Resistant Acinetobacter baumannii I...\n",
              "87328                                                      What did NJDOH investigate?  ...  Increase in Hospital-Acquired Carbapenem-Resistant Acinetobacter baumannii I...\n",
              "87329                                         What led to deviations in IPC practices?  ...  Increase in Hospital-Acquired Carbapenem-Resistant Acinetobacter baumannii I...\n",
              "87330                             What could hospitals with COVID-19 be vulnerable to?  ...  Increase in Hospital-Acquired Carbapenem-Resistant Acinetobacter baumannii I...\n",
              "\n",
              "[87331 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSXz82iMiYZx"
      },
      "source": [
        "topK = 10 # Get top 10 best matching documents as the hard negatives \n",
        "qa_FN_DPR_format = [] # The DPR format above is json like \n",
        "for idx, r in qa.iterrows(): # Iterate each synthetic QA pair in QA.csv and convert into DPR data format above \n",
        "  question = qa.question[idx]\n",
        "  question = preprocess(question) \n",
        "  docScores = BM25.get_scores(question)\n",
        "  # docScores = np.delete(docScores, idx) # Remove the context to current question (so we dont pick it)\n",
        "  idxTopKDocs = np.argsort(docScores)[::-1][:topK] # Reverse order and get best ones (best hard negatives)\n",
        "  hard_negatives = qa.iloc[idxTopKDocs]\n",
        "\n",
        "  positive_ctxs = {\"title\": r.title, \"text\": r.context}\n",
        "  hard_negative_ctxs = [{\"title\": r.title, \"text\": r.context} for _, r in hard_negatives.iterrows()]\n",
        "  qa_FN_DPR_format.append({\"question\": r.question, \"answers\": [r.answer], \"positive_ctxs\": [positive_ctxs], \"negative_ctxs\": [], \"hard_negative_ctxs\": hard_negative_ctxs})\n",
        "\n",
        "with open(\"qa_FN_DPR_format.json\", \"w\") as fout:\n",
        "  json.dump(qa_FN_DPR_format, fout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYt0Ljstlq2c"
      },
      "source": [
        "# Splits data into training, dev, test for DPR fine-tuning \n",
        "split = math.floor(len(qa_FN_DPR_format)*0.8) \n",
        "\n",
        "with open(\"qa_FN_DPR_format_TRAIN.json\", \"w\") as fout:\n",
        "  json.dump(qa_FN_DPR_format[0:split], fout)\n",
        "\n",
        "with open(\"qa_FN_DPR_format_DEV.json\", \"w\") as fout:\n",
        "  json.dump(qa_FN_DPR_format[split:], fout)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzgWIG20zcN-"
      },
      "source": [
        "files.download(\"qa_FN_DPR_format.json\")\n",
        "files.download(\"qa_FN_DPR_format_TRAIN.json\")\n",
        "files.download(\"qa_FN_DPR_format_DEV.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLxyiIok9z83"
      },
      "source": [
        "# Training and Fine-tuning the DPR\n",
        "\n",
        "Now that the data has been processed, we can train the DPR!\n",
        "\n",
        "The above code preprocesses the data for DPR training, but I have already saved this output to my google drive, so the following two lines again downloads the preprocessed data for DPR training so we dont have to wait 5 hrs for the above code lol."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8tu6ZXuns4n"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqTASbnUnvYm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOZi6DUN-Y0s"
      },
      "source": [
        "# Download the processed data in the json format requried for DPR\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1DG4sa6eCHuVCiuQqRtdk-YfYqezEj7rA' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1DG4sa6eCHuVCiuQqRtdk-YfYqezEj7rA\" -O qa_FN_DPR_format_TRAIN.json && rm -rf /tmp/cookies.txt\n",
        "\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1bxsfG-oZBJVG9qVCG9lCO2AH-h2lsR8C' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1bxsfG-oZBJVG9qVCG9lCO2AH-h2lsR8C\" -O qa_FN_DPR_format_DEV.json && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqfGD9fijHOX"
      },
      "source": [
        "# Fine tuning\n",
        "train_filename = \"qa_FN_DPR_format_TRAIN.json\"\n",
        "dev_filename = \"qa_FN_DPR_format_DEV.json\"\n",
        "\n",
        "query_model = \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "passage_model = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "\n",
        "save_dir = \"/dpr/saved_models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBzYgJfjDfS"
      },
      "source": [
        "## Initialize DPR model\n",
        "\n",
        "retriever = DensePassageRetriever(\n",
        "    document_store=InMemoryDocumentStore(),\n",
        "    query_embedding_model=query_model,\n",
        "    passage_embedding_model=passage_model,\n",
        "    max_seq_len_query=64,\n",
        "    max_seq_len_passage=256\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paS2iTy9FIf4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DPv1YLSjO9X"
      },
      "source": [
        "# Start training our model and save it when it is finished\n",
        "retriever.train(\n",
        "    data_dir=\"\",\n",
        "    train_filename=train_filename,\n",
        "    dev_filename=dev_filename,\n",
        "    test_filename=dev_filename,\n",
        "    n_epochs=1,\n",
        "    batch_size=16,\n",
        "    grad_acc_steps=8,\n",
        "    save_dir=save_dir,\n",
        "    evaluate_every=3000,\n",
        "    embed_title=True,\n",
        "    num_positives=1,\n",
        "    num_hard_negatives=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUvl0IFGY_ny"
      },
      "source": [
        "Zip the DPR model folder so we can export/download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWcHqAQ3wB5f"
      },
      "source": [
        "!zip -r dpr.zip /dpr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3s4K6IxwtA7"
      },
      "source": [
        "# Download fine-tuned DPR model and save to local machine\n",
        "files.download(\"dpr.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-KXkxn7ZMAI"
      },
      "source": [
        "# Reloading Fine-Tuned DPR Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo3nzZ3bjQ48"
      },
      "source": [
        "# Reload fine-tuned DPR \n",
        "reloaded_retriever = DensePassageRetriever.load(load_dir = \"dpr/saved_models\", document_store = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TywmQgnCHXyt"
      },
      "source": [
        "# QA Pipeline Using Fine-tuned DPR + RAG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihl3uld6Ysen"
      },
      "source": [
        "# Download the fine-tuned DPR \n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1--TphsSGk083rMNzSoEW6GaK_Qm8fiRe' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1--TphsSGk083rMNzSoEW6GaK_Qm8fiRe\" -O dpr.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Unzip the dowloaded DPR\n",
        "! unzip -q dpr.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHcAFV6SLeVe"
      },
      "source": [
        "# Download the COVID-QA data \n",
        "!wget https://raw.githubusercontent.com/deepset-ai/COVID-QA/master/data/question-answering/COVID-QA.json COVID-QA.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Ru6781Mqvl"
      },
      "source": [
        "COVID_QA = json.load(open(\"COVID-QA.json\"))\n",
        "len(COVID_QA[\"data\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHnOCLwPzKwV"
      },
      "source": [
        "# Convert json object into a pandas dataframe \n",
        "qas = []\n",
        "documents = []\n",
        "for data in COVID_QA[\"data\"]:\n",
        "  qas.append(data[\"paragraphs\"][0])\n",
        "  documents.append({\"text\": data[\"paragraphs\"][0][\"context\"], \"meta\": {\"name\": data[\"paragraphs\"][0][\"document_id\"]}})\n",
        "COVID_QA_df = pd.DataFrame(qas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE7uxCvMHaKy"
      },
      "source": [
        "# Initialize FAISS document store.\n",
        "# Set `return_embedding` to `True`, so generator doesn't have to perform re-embedding\n",
        "document_store = FAISSDocumentStore(\n",
        "    faiss_index_factory_str=\"Flat\",\n",
        "    return_embedding=True\n",
        ")\n",
        "\n",
        "# Initialize DPR Retriever to encode documents, encode question and query documents\n",
        "retriever = DensePassageRetriever.load(\n",
        "    load_dir = \"dpr/saved_models\", document_store = document_store\n",
        ")\n",
        "\n",
        "# Initialize RAG Generator\n",
        "generator = RAGenerator(\n",
        "    model_name_or_path=\"facebook/rag-token-nq\",\n",
        "    use_gpu=True,\n",
        "    top_k=1,\n",
        "    max_length=200,\n",
        "    min_length=2,\n",
        "    embed_title=True,\n",
        "    num_beams=2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsz0NrzcN--I"
      },
      "source": [
        "# Delete existing documents in documents store\n",
        "document_store.delete_documents()\n",
        "\n",
        "# Write documents to document store\n",
        "document_store.write_documents(documents)\n",
        "\n",
        "# Add documents embeddings to index\n",
        "document_store.update_embeddings(\n",
        "    retriever=retriever\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aujr3U-bF1SF"
      },
      "source": [
        "#Evaluating the retrieval recall\n",
        "for question in QUESTIONS:\n",
        "    # Retrieve related documents from retriever\n",
        "    retriever_results = retriever.retrieve(\n",
        "        query=question\n",
        "    )\n",
        "\n",
        "    # Now generate answer from question and retrieved documents\n",
        "    predicted_result = generator.predict(\n",
        "        query=question,\n",
        "        documents=retriever_results,\n",
        "        top_k=1\n",
        "    )\n",
        "\n",
        "    # Print you answer\n",
        "    answers = predicted_result[\"answers\"]\n",
        "    print(f'Generated answer is \\'{answers[0][\"answer\"]}\\' for the question = \\'{question}\\'')\n",
        "#https://haystack.deepset.ai/docs/latest/tutorial7md\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0y6mMdizrj5"
      },
      "source": [
        "# Or alternatively use the Pipeline class\n",
        "from haystack.pipeline import GenerativeQAPipeline\n",
        "pipe = GenerativeQAPipeline(generator=generator, retriever=retriever)\n",
        "res = []\n",
        "for idx, data in COVID_QA_df.iterrows():\n",
        "  for qas in data[\"qas\"]:\n",
        "    ans = pipe.run(query=qas[\"question\"], top_k_generator=1, top_k_retriever=5)\n",
        "    # res.append({\"question\": qas[\"question\"], \"true_answers\": [ans[\"text\"] for ans in qas[\"answers\"]], \"pred_answer\": ans})\n",
        "    res.append(ans)\n",
        "res_df = pd.DataFrame(res)\n",
        "print(res_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWOD4xYAksMs"
      },
      "source": [
        "res_df.to_csv(\"DPR_RAG_COVID_QA.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IBdG4gXizbQ"
      },
      "source": [
        "# **BELOW IS OLD CODE JUST KEEPING FOR REFERENCE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b43_-ShoOrt"
      },
      "source": [
        "In the DPR training notebook, we download these files to train the DPR from scratch\n",
        "\n",
        "s3_url_train = \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz\"\n",
        "\n",
        "s3_url_dev = \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\"\n",
        "\n",
        "Unzipping the first file we get \"biencoder-nq-train.json\"\n",
        "\n",
        "Reading the file with the following script gives us the JSON object format for training/fine tuning the DPR. This is just for reference to get an idea of file format required for training/fine tuning DPR. \n",
        "\n",
        "```python\n",
        "import json\n",
        "infile = \"biencoder-nq-train.json\"\n",
        "count = 0 \n",
        "with open(infile) as f_read:\n",
        "    for line in f_read:\n",
        "        if count > 100:\n",
        "            break\n",
        "        line = line.strip()\n",
        "        if len(line) > 0:\n",
        "            print(line)\n",
        "        count = count + 1\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "[\n",
        "{\n",
        "\"dataset\": \"nq_train_psgs_w100\",\n",
        "\"question\": \"big little lies season 2 how many episodes\",\n",
        "\"answers\": [\n",
        "\"seven\"\n",
        "],\n",
        "\"positive_ctxs\": [\n",
        "{\n",
        "\"title\": \"Big Little Lies (TV series)\",\n",
        "\"text\": \"series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley\",   \n",
        "\"score\": 1000,\n",
        "\"title_score\": 1,\n",
        "\"passage_id\": \"18768923\"\n",
        "},\n",
        "{\n",
        "\"title\": \"Little People, Big World\",\n",
        "\"text\": \"TLC announced a spin-off series \\u2013 \\\"\\\". It chronicles Matt and Amy as they jump-start their wedding business on the farm. The series premiered on November 13, 2012, and ran for six episodes. It was announced in October 2013 that \\\"Little People, Big World\\\" would return for a seventh season. Season seven consists of eight episodes, and premiered on October 29, 2013. An eighth season began on September 2, 2014, \n",
        "and a ninth season began on July 6, 2015. The show was renewed for a tenth season which premiered in fall 2016. As of 2018, the show has aired for\",\n",
        "\"score\": 13.371864,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"7459110\"\n",
        "},\n",
        "{\n",
        "\"title\": \"Big Little Lies (TV series)\",\n",
        "\"text\": \"shows of 2017. A soundtrack for the series was released on Google Play and iTunes on March 31, 2017. The first season was released on Blu-ray and DVD on August 1, 2017. Big Little Lies (TV series) Big Little Lies is an American drama television series, based on the novel of the same name by Liane Moriarty, that premiered on February 19, 2017, on HBO. Created and written by David E. Kelley, the series' seven-episode \n",
        "first season was directed by Jean-Marc Vall\\u00e9e. \\\"Big Little Lies\\\" stars Nicole Kidman, Reese Witherspoon and Shailene Woodley and tells the story of three emotionally\",\n",
        "\"score\": 12.932647,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"18768935\"\n",
        "},\n",
        "{\n",
        "\"title\": \"Big Little Lies (TV series)\",\n",
        "\"text\": \"Big Little Lies (TV series) Big Little Lies is an American drama television series, based on the novel of the same name by Liane Moriarty, that premiered on February 19, 2017, on HBO. Created and written by David E. Kelley, the series' seven-episode first season was directed by Jean-Marc Vall\\u00e9e. \\\"Big Little Lies\\\" stars Nicole Kidman, Reese Witherspoon and Shailene Woodley and tells the story of three emotionally troubled women in Monterey, California, who become embroiled in a murder investigation. Alexander Skarsg\\u00e5rd, Laura Dern, Jeffrey Nordling, Adam Scott, Zo\\u00eb Kravitz, and James Tupper feature in supporting roles. Critically acclaimed, the\",\n",
        "\"score\": 12.449134,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"18768922\"\n",
        "},\n",
        "{\n",
        "\"title\": \"Andrea Arnold\",\n",
        "\"text\": \"Andrea Arnold Andrea Arnold, OBE (born 5 April 1961) is an English filmmaker and former actress. She won an Academy Award for her short film \\\"Wasp\\\" in 2005. She has since made the leap to feature films and television, including \\\"Red Road\\\" (2006), \\\"Fish Tank\\\" (2009), and \\\"American Honey\\\" (2016), all of which have won the Jury Prize at the Cannes Film Festival. Arnold has also directed four episodes of the Emmy Award-winning series \\\"Transparent\\\", as well as all seven episodes of the second season of the Emmy Award-winning series \\\"Big Little Lies\\\". Arnold was born in Dartford, Kent, the\",\n",
        "\"score\": 12.204561,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"7854255\"\n",
        "},\n",
        "{\n",
        "\"title\": \"Designing Women\",\n",
        "\"text\": \"to whom she eventually loses. In reality, Dixie Carter was a Republican who disagreed with some of the liberal views expressed by her onscreen character, although she did become a Clinton supporter. Shout! Factory has released all seven seasons of \\\"Designing Women\\\" on DVD in Region 1. On September 2, 2003, Sony Pictures released \\\"The Best of Designing Women\\\", a single-disc DVD featuring five episodes ranging between seasons one through four: \\\"Designing Women (Pilot)\\\" (season 1), \\\"Killing All the Right People\\\" (season 2), \\\"Reservations for Eight\\\" (season 2), \\\"Big Haas and Little Falsie\\\" (season 3) and \\\"They Shoot \n",
        "Fat Women,\",\n",
        "\"score\": 11.899436,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"1523654\"\n",
        "},\n",
        "{\n",
        "\"title\": \"Big Little Lies (TV series)\",\n",
        "\"text\": \"series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley\",   \n",
        "\"score\": 11.830096,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"18768923\"\n",
        "},\n",
        "{\n",
        "\"title\": \"The X-Files (season 2)\",\n",
        "\"text\": \"out of five to \\\"Little Green Men\\\", \\\"Duane Barry\\\", \\\"One Breath\\\", \\\"Irresistible\\\", \\\"Die Hand Die Verletzt\\\", and \\\"Anasazi\\\". However, several episodes rated poorly, with \\\"3\\\", \\\"Excelsis Dei\\\", and \\\"The Calusari\\\" being considered particularly poor. Many critics considered the \\\"Duane Barry\\\"/\\\"Ascension\\\"/\\\"One Breath\\\" story arc to be the best part of the season. Shearman singled out the three-parter as the highlight of the season, noting that the \\\"intimacy\\\" and \\\"sincerity [of] the emotion\\\" of the episodes allowed the mythology of \\\"The X-Files\\\" to play out for a further seven seasons. Tom Kessenich, in his book \\\"Examination: An Unauthorized Look at Seasons 6\\u20139\",\n",
        "\"score\": 10.869938,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"3670320\"\n",
        "},\n",
        "{\n",
        "\"title\": \"The Big Bang Theory\",\n",
        "\"text\": \"season. The second half of season seven aired in mid 2014. The eighth season premiered on E4 on October 23, 2014 at 8:30 p.m. During its eighth season, \\\"The Big Bang Theory\\\" shared its 8:30 p.m. time period with fellow CBS comedy, \\\"2 Broke Girls\\\". Following the airing of the first eight episodes of that show's fourth season, \\\"The Big Bang Theory\\\" returned to finish airing its eighth season on March 19, 2015. Netflix UK & Ireland announced on February 13, 2016 that seasons 1\\u20138 would be available to stream from February 15, 2016. \\\"The Big Bang Theory\\\" started off\",\n",
        "\"score\": 10.384873,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"10248582\"\n",
        "}\n",
        "],\n",
        "\"negative_ctxs\": [\n",
        "{\n",
        "\"title\": \"Cormac McCarthy\",\n",
        "\"text\": \"chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \\\"The New York Times\\\", \\\"McCarthy doesn't drink anymore \\u2013 he quit 16 years ago in El Paso, with one of his young\",\n",
        "\"score\": 0,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"2145653\"\n",
        "},\n",
        "{\n",
        "\"title\": \"Pragmatic Sanction of 1549\",\n",
        "\"text\": \"one heir, Charles effectively united the Netherlands as one entity. After Charles' abdication in 1555, the Seventeen Provinces passed to his son, Philip II of Spain. The Pragmatic Sanction is said to be one example of the Habsburg contest with particularism that contributed to the Dutch Revolt. Each of the provinces had its own laws, customs and political practices. The new policy, imposed from the outside, angered \n",
        "many inhabitants, who viewed their provinces as distinct entities. It and other monarchical acts, such as the creation of bishoprics and promulgation of laws against heresy, stoked resentments, which fired the eruption of\",\n",
        "\"score\": 0,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"2271902\"\n",
        "},\n",
        "{\n",
        "\"title\": \"Hall Airport\",\n",
        "\"text\": \"Hall Airport Hall Airport is a privately owned, public use airport located six nautical miles (11 km) northwest of the central business district of Kaufman, a city in Kaufman County, Texas, United States. \n",
        "Hall Airport covers an area of 27 acres (11 ha) at an elevation of 440 feet (134 m) above mean sea level. It has one runway designated 17/35 with a turf surface measuring 2,585 by 40 feet (788 x 12 m). For the 12-month period ending May 23, 2007, the airport had 201 general aviation aircraft operations, an average of 16 per month. At that time there\",\n",
        "\"score\": 0,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"17333840\"\n",
        "},\n",
        "{\n",
        "\"title\": \"St Columba's College, Largs\",\n",
        "\"text\": \"early 1960s) who was followed by Brother Germanus (who later became David Germanus) then Brother Nicholas (who later left the Brothers to get married) who is mentioned as part of a 'holy fourball' of golfers (given his golfing skills) on pages 9 and 15 of \\\"Sam\\\" the autiobiography of Sam Torrance, the famous golfer and team captain for Europe in the Ryder Cup in 2002. Sam was a member, as was his father the Club Professional, of Routenburn Golf Club, situated above and adjacent to the school. The various headmasters kept up the wonderful traditions of devotion to the Marist\",\n",
        "\"score\": 0,\n",
        "\"title_score\": 0,\n",
        "\"passage_id\": \"13284527\"\n",
        "},\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoZNEy2uoLyO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLdVOohRpSXX"
      },
      "source": [
        "# topK = 10\n",
        "# fn = []\n",
        "# for r in qa.index: \n",
        "#   question = qa.question[r]\n",
        "#   question = preprocess(question) \n",
        "#   docScores = BM25.get_scores(question)\n",
        "#   docScores = np.delete(docScores, r)\n",
        "#   idxTopKDocs = np.argsort(docScores)[::-1][:topK] # Reverse order and get best ones\n",
        "#   fn.append(qa.context[idxTopKDocs].tolist())\n",
        "# qa[\"False Negatives\"] = fn\n",
        "# qa.to_csv(\"QA_FN.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuq8XzR7b5zO"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1qz6Nv95mxhus03IqNnwbrxXu9m-dZI3X' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1qz6Nv95mxhus03IqNnwbrxXu9m-dZI3X\" -O QA_FN.csv && rm -rf /tmp/cookies.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqUGA7MwcIfV"
      },
      "source": [
        "# qa_FN = pd.read_csv(\"QA_FN.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgP6c3D_wpIu"
      },
      "source": [
        "The following code takes a reduced qa dataframe and puts the data into the dictionary format that is required for the DPR training model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe_muphJeuCC"
      },
      "source": [
        "# # Creating a reduced qa dataframe to try use DPR training model with\n",
        "# reduced_qa = qa.iloc[0:100,:]\n",
        "\n",
        "# topK = 10\n",
        "# fn = []\n",
        "# p_ctxs = []\n",
        "# for i in range(100):\n",
        "#   question = qa.question[i]\n",
        "#   question = preprocess(question) \n",
        "#   docScores = BM25.get_scores(question)\n",
        "#   p_ctxs.append({\"title\":\"Nothing\", \"text\":qa.context[i], \"score\":docScores[i], \"title_score\":0, \"passage_id\":i})\n",
        "#   docScores = np.delete(docScores, i)\n",
        "#   idxTopKDocs = np.argsort(docScores)[::-1][:topK] # Reverse order and get best ones\n",
        "#   pre_fn = [{\"title\":\"Nothing\", \"text\":qa.context[x], \"score\":docScores[x], \"title_score\":0, \"passage_id\":x} for x in idxTopKDocs] # Store each false negative using a dictionary structure\n",
        "#   fn.append(pre_fn)\n",
        "\n",
        "# reduced_qa[\"hard_negative_ctxs\"] = fn\n",
        "# reduced_qa[\"positive_ctxs\"] = p_ctxs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA1CLgjng4tJ"
      },
      "source": [
        "# Install the latest master of Haystack\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I0h9YSng8wh"
      },
      "source": [
        "from haystack.retriever.dense import DensePassageRetriever\n",
        "from haystack.preprocessor.utils import fetch_archive_from_http\n",
        "from haystack.document_store.memory import InMemoryDocumentStore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMsPt_QajFtK"
      },
      "source": [
        "# df = []\n",
        "# for i in qa.index:\n",
        "#   df.append({\"dataset\":i,\"question\":qa.question[i],\"answers\":qa.answer[i].split(\" \"), \"positive_ctxs\":qa.positive_ctxs[i], \"negative_ctxs\":[], \"hard_negative_ctxs\":qa.hard_negative_ctxs[i]})\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}