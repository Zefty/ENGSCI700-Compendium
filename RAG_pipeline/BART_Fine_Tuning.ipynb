{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BART Fine-Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxCxbwL85HAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c333acf6-8ce0-4ecb-8067-32bd1dec68b8"
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1kKZSBpgDwRCvaMR9y9caEs1wSdbFKRzs' -O QA.csv # Downloading qa.csv from google drive\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/summarization/run_summarization.py -O run_summarization.py \n",
        "\n",
        "# Installing the libraries that run_summarization.py uses\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install rouge_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-06 05:32:23--  https://docs.google.com/uc?export=download&id=1kKZSBpgDwRCvaMR9y9caEs1wSdbFKRzs\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.33.206, 2607:f8b0:4004:837::200e\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.33.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tdfn1caq8950erb092r9i2pa5img6skt/1625549550000/03976892977300334194/*/1kKZSBpgDwRCvaMR9y9caEs1wSdbFKRzs?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-07-06 05:32:32--  https://doc-0s-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tdfn1caq8950erb092r9i2pa5img6skt/1625549550000/03976892977300334194/*/1kKZSBpgDwRCvaMR9y9caEs1wSdbFKRzs?e=download\n",
            "Resolving doc-0s-bk-docs.googleusercontent.com (doc-0s-bk-docs.googleusercontent.com)... 142.250.188.193, 2607:f8b0:4004:836::2001\n",
            "Connecting to doc-0s-bk-docs.googleusercontent.com (doc-0s-bk-docs.googleusercontent.com)|142.250.188.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘QA.csv’\n",
            "\n",
            "QA.csv                  [    <=>             ]  81.63M   130MB/s    in 0.6s    \n",
            "\n",
            "2021-07-06 05:32:33 (130 MB/s) - ‘QA.csv’ saved [85597679]\n",
            "\n",
            "--2021-07-06 05:32:33--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/summarization/run_summarization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26428 (26K) [text/plain]\n",
            "Saving to: ‘run_summarization.py’\n",
            "\n",
            "run_summarization.p 100%[===================>]  25.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-06 05:32:33 (117 MB/s) - ‘run_summarization.py’ saved [26428/26428]\n",
            "\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/27/9c91ddee87b06d2de12f134c5171a49890427e398389f07f6463485723c3/datasets-1.9.0-py3-none-any.whl (262kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 29.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.5.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting fsspec>=2021.05.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/35/03/071adc023c0a7e540cf4652fa9cad13ab32e6ae469bf0cc0262045244812/huggingface_hub-0.0.13-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, fsspec, huggingface-hub, datasets\n",
            "Successfully installed datasets-1.9.0 fsspec-2021.6.1 huggingface-hub-0.0.13 xxhash-2.0.2\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-nkvms0ob\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-nkvms0ob\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 27.9MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (4.5.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.9.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.9.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.9.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (3.0.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.9.0.dev0-cp37-none-any.whl size=2534484 sha256=e42aad7c494514d96bf6d5a98998d35530b79fc0d4e3ae78283af2dc28ae80a7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yklidmlq/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n",
            "  Found existing installation: huggingface-hub 0.0.13\n",
            "    Uninstalling huggingface-hub-0.0.13:\n",
            "      Successfully uninstalled huggingface-hub-0.0.13\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.0.dev0\n",
            "Collecting rouge_score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Nvak6m9vU_"
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOy7ZA1I6r4h"
      },
      "source": [
        "qa = pd.read_csv(\"QA.csv\")\n",
        "qa = qa.dropna()\n",
        "\n",
        "# Creates a split so that we have 60% training and 40% validation\n",
        "split = math.floor(len(qa.index)*0.6)\n",
        "\n",
        "# Put qa.csv data into required format for run_summarization.py\n",
        "text_column = []\n",
        "summary_column = []\n",
        "for i in range(len(qa.index)):\n",
        "  text_column.append(qa[\"context\"].values[i]+\"<q>\"+qa[\"question\"].values[i])\n",
        "  summary_column.append(str(qa[\"answer\"].values[i])+\"\")\n",
        "\n",
        "train = {\"text_column\":text_column[0:split], \"summary_column\":summary_column[0:split]}\n",
        "train = pd.DataFrame.from_dict(train)\n",
        "train.to_csv(\"qa_bart_train_format.csv\", index=False)\n",
        "\n",
        "eval = {\"text_column\":text_column[split:], \"summary_column\":summary_column[split:]}\n",
        "eval = pd.DataFrame.from_dict(eval)\n",
        "eval.to_csv(\"qa_bart_eval_format.csv\", index=False)\n",
        "\n",
        "# files.download(\"qa_bart_train_format.csv\")\n",
        "# files.download(\"qa_bart_eval_format.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uieiarH-tWFV",
        "outputId": "8c8dd591-a632-4a77-9d27-a35649b41bb9"
      },
      "source": [
        "!python run_summarization.py \\\n",
        "    --model_name_or_path facebook/bart-base \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_file qa_bart_train_format.csv \\\n",
        "    --validation_file qa_bart_eval_format.csv \\\n",
        "    --source_prefix \"summarize: \" \\\n",
        "    --output_dir /tmp/tst-summarization \\\n",
        "    --overwrite_output_dir \\\n",
        "    --save_total_limit=5\\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --predict_with_generate \\\n",
        "    --text_column text_column \\\n",
        "    --summary_column summary_column"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-04 21:33:39.380667: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/04/2021 21:33:40 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/04/2021 21:33:40 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/tst-summarization/runs/Jul04_21-33-40_730e6194f62c,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/tmp/tst-summarization,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=tst-summarization,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/tst-summarization,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/04/2021 21:33:40 - WARNING - datasets.builder - Using custom data configuration default-a42ba5c879e6138f\n",
            "07/04/2021 21:33:40 - INFO - datasets.utils.filelock - Lock 140247163229968 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_csv_default-a42ba5c879e6138f_0.0.0_2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0.lock\n",
            "07/04/2021 21:33:40 - INFO - datasets.utils.filelock - Lock 140247163229968 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_csv_default-a42ba5c879e6138f_0.0.0_2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0.lock\n",
            "07/04/2021 21:33:40 - INFO - datasets.utils.filelock - Lock 140247163230160 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_csv_default-a42ba5c879e6138f_0.0.0_2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0.lock\n",
            "07/04/2021 21:33:40 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-a42ba5c879e6138f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-a42ba5c879e6138f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
            "100% 2/2 [00:00<00:00, 9489.38it/s]\n",
            "07/04/2021 21:33:40 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "07/04/2021 21:33:41 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 2/2 [00:00<00:00, 1284.04it/s]\n",
            "07/04/2021 21:33:41 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/04/2021 21:33:41 - INFO - datasets.builder - Generating split train\n",
            "07/04/2021 21:33:41 - INFO - datasets.arrow_writer - Done writing 52398 examples in 47148418 bytes /root/.cache/huggingface/datasets/csv/default-a42ba5c879e6138f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0.incomplete/csv-train.arrow.\n",
            "07/04/2021 21:33:41 - INFO - datasets.builder - Generating split validation\n",
            "07/04/2021 21:33:42 - INFO - datasets.arrow_writer - Done writing 34932 examples in 30568812 bytes /root/.cache/huggingface/datasets/csv/default-a42ba5c879e6138f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0.incomplete/csv-validation.arrow.\n",
            "07/04/2021 21:33:42 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "07/04/2021 21:33:42 - INFO - datasets.utils.filelock - Lock 140247163231440 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_csv_default-a42ba5c879e6138f_0.0.0_2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0.incomplete.lock\n",
            "07/04/2021 21:33:42 - INFO - datasets.utils.filelock - Lock 140247163231440 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_csv_default-a42ba5c879e6138f_0.0.0_2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0.incomplete.lock\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-a42ba5c879e6138f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
            "07/04/2021 21:33:42 - INFO - datasets.utils.filelock - Lock 140247163230160 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_csv_default-a42ba5c879e6138f_0.0.0_2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0.lock\n",
            "07/04/2021 21:33:42 - INFO - datasets.builder - Constructing Dataset for split train, validation, from /root/.cache/huggingface/datasets/csv/default-a42ba5c879e6138f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0\n",
            "100% 2/2 [00:00<00:00, 365.56it/s]\n",
            "[INFO|file_utils.py:1590] 2021-07-04 21:33:42,160 >> https://huggingface.co/facebook/bart-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpe65qdx5g\n",
            "Downloading: 100% 1.63k/1.63k [00:00<00:00, 2.25MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-07-04 21:33:42,182 >> storing https://huggingface.co/facebook/bart-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.8512cdf8592f538a7fd4b40eecaa096285410ec6494049568b3300922ab71165\n",
            "[INFO|file_utils.py:1602] 2021-07-04 21:33:42,182 >> creating metadata file for /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.8512cdf8592f538a7fd4b40eecaa096285410ec6494049568b3300922ab71165\n",
            "[INFO|configuration_utils.py:537] 2021-07-04 21:33:42,183 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.8512cdf8592f538a7fd4b40eecaa096285410ec6494049568b3300922ab71165\n",
            "[INFO|configuration_utils.py:573] 2021-07-04 21:33:42,183 >> Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:430] 2021-07-04 21:33:42,205 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:537] 2021-07-04 21:33:42,226 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.8512cdf8592f538a7fd4b40eecaa096285410ec6494049568b3300922ab71165\n",
            "[INFO|configuration_utils.py:573] 2021-07-04 21:33:42,227 >> Model config BartConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1590] 2021-07-04 21:33:42,249 >> https://huggingface.co/facebook/bart-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5b3hds39\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 45.4MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-07-04 21:33:42,299 >> storing https://huggingface.co/facebook/bart-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|file_utils.py:1602] 2021-07-04 21:33:42,299 >> creating metadata file for /root/.cache/huggingface/transformers/43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|file_utils.py:1590] 2021-07-04 21:33:42,323 >> https://huggingface.co/facebook/bart-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbggrkex1\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 43.1MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-07-04 21:33:42,358 >> storing https://huggingface.co/facebook/bart-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1602] 2021-07-04 21:33:42,358 >> creating metadata file for /root/.cache/huggingface/transformers/3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1590] 2021-07-04 21:33:42,382 >> https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9qciuknl\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 45.9MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-07-04 21:33:42,447 >> storing https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|file_utils.py:1602] 2021-07-04 21:33:42,447 >> creating metadata file for /root/.cache/huggingface/transformers/a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-04 21:33:42,520 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-04 21:33:42,520 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-04 21:33:42,520 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-04 21:33:42,520 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-04 21:33:42,520 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-04 21:33:42,520 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|file_utils.py:1590] 2021-07-04 21:33:42,603 >> https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbo_b22s6\n",
            "Downloading: 100% 558M/558M [00:11<00:00, 47.4MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-07-04 21:33:54,618 >> storing https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.9faea28a6782a9589c09b1942c039943df02232d83d2ac288a69ddfa928eae22\n",
            "[INFO|file_utils.py:1602] 2021-07-04 21:33:54,618 >> creating metadata file for /root/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.9faea28a6782a9589c09b1942c039943df02232d83d2ac288a69ddfa928eae22\n",
            "[INFO|modeling_utils.py:1237] 2021-07-04 21:33:54,618 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.9faea28a6782a9589c09b1942c039943df02232d83d2ac288a69ddfa928eae22\n",
            "[INFO|modeling_utils.py:1444] 2021-07-04 21:33:56,449 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1453] 2021-07-04 21:33:56,449 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on train dataset:   0% 0/53 [00:00<?, ?ba/s]07/04/2021 21:33:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-a42ba5c879e6138f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-214e082da57cc905.arrow\n",
            "Running tokenizer on train dataset: 100% 53/53 [00:19<00:00,  2.78ba/s]\n",
            "07/04/2021 21:34:15 - INFO - datasets.arrow_writer - Done writing 52398 examples in 52666064 bytes /root/.cache/huggingface/datasets/csv/default-a42ba5c879e6138f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/tmpcud7jzk3.\n",
            "Running tokenizer on validation dataset:   0% 0/35 [00:00<?, ?ba/s]07/04/2021 21:34:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-a42ba5c879e6138f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-c553b011bea2de91.arrow\n",
            "Running tokenizer on validation dataset: 100% 35/35 [00:12<00:00,  2.83ba/s]\n",
            "07/04/2021 21:34:28 - INFO - datasets.arrow_writer - Done writing 34932 examples in 34203782 bytes /root/.cache/huggingface/datasets/csv/default-a42ba5c879e6138f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/tmphscd1z8g.\n",
            "07/04/2021 21:34:28 - INFO - datasets.utils.filelock - Lock 140250526599056 acquired on /root/.cache/huggingface/datasets/downloads/d1add26d91c8e217931d0a66e302493b59f00358792a4953327d8e8fca724da3.0176a9ea14f2e537f6dfc6a072a422e7075a0cf475bb3b71e493addc7d8dde62.py.lock\n",
            "07/04/2021 21:34:28 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/rouge/rouge.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp98atmj0j\n",
            "Downloading: 5.61kB [00:00, 5.90MB/s]       \n",
            "07/04/2021 21:34:28 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/rouge/rouge.py in cache at /root/.cache/huggingface/datasets/downloads/d1add26d91c8e217931d0a66e302493b59f00358792a4953327d8e8fca724da3.0176a9ea14f2e537f6dfc6a072a422e7075a0cf475bb3b71e493addc7d8dde62.py\n",
            "07/04/2021 21:34:28 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d1add26d91c8e217931d0a66e302493b59f00358792a4953327d8e8fca724da3.0176a9ea14f2e537f6dfc6a072a422e7075a0cf475bb3b71e493addc7d8dde62.py\n",
            "07/04/2021 21:34:28 - INFO - datasets.utils.filelock - Lock 140250526599056 released on /root/.cache/huggingface/datasets/downloads/d1add26d91c8e217931d0a66e302493b59f00358792a4953327d8e8fca724da3.0176a9ea14f2e537f6dfc6a072a422e7075a0cf475bb3b71e493addc7d8dde62.py.lock\n",
            "07/04/2021 21:34:28 - INFO - datasets.load - Checking /root/.cache/huggingface/datasets/downloads/d1add26d91c8e217931d0a66e302493b59f00358792a4953327d8e8fca724da3.0176a9ea14f2e537f6dfc6a072a422e7075a0cf475bb3b71e493addc7d8dde62.py for additional imports.\n",
            "07/04/2021 21:34:28 - INFO - datasets.utils.filelock - Lock 140249811908944 acquired on /root/.cache/huggingface/datasets/downloads/d1add26d91c8e217931d0a66e302493b59f00358792a4953327d8e8fca724da3.0176a9ea14f2e537f6dfc6a072a422e7075a0cf475bb3b71e493addc7d8dde62.py.lock\n",
            "07/04/2021 21:34:28 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/rouge/rouge.py at /root/.cache/huggingface/modules/datasets_modules/metrics/rouge\n",
            "07/04/2021 21:34:28 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/rouge/rouge.py at /root/.cache/huggingface/modules/datasets_modules/metrics/rouge/2b73d5eb463209373e9d21a95decb226d4164bdca4c361b8dfad295ec82bc62e\n",
            "07/04/2021 21:34:28 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/rouge/rouge.py to /root/.cache/huggingface/modules/datasets_modules/metrics/rouge/2b73d5eb463209373e9d21a95decb226d4164bdca4c361b8dfad295ec82bc62e/rouge.py\n",
            "07/04/2021 21:34:28 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/rouge/dataset_infos.json\n",
            "07/04/2021 21:34:28 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.8.0/metrics/rouge/rouge.py at /root/.cache/huggingface/modules/datasets_modules/metrics/rouge/2b73d5eb463209373e9d21a95decb226d4164bdca4c361b8dfad295ec82bc62e/rouge.json\n",
            "07/04/2021 21:34:28 - INFO - datasets.utils.filelock - Lock 140249811908944 released on /root/.cache/huggingface/datasets/downloads/d1add26d91c8e217931d0a66e302493b59f00358792a4953327d8e8fca724da3.0176a9ea14f2e537f6dfc6a072a422e7075a0cf475bb3b71e493addc7d8dde62.py.lock\n",
            "[INFO|trainer.py:1153] 2021-07-04 21:34:40,229 >> ***** Running training *****\n",
            "[INFO|trainer.py:1154] 2021-07-04 21:34:40,229 >>   Num examples = 52398\n",
            "[INFO|trainer.py:1155] 2021-07-04 21:34:40,229 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1156] 2021-07-04 21:34:40,229 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1157] 2021-07-04 21:34:40,229 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1158] 2021-07-04 21:34:40,230 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1159] 2021-07-04 21:34:40,230 >>   Total optimization steps = 78597\n",
            "{'loss': 1.1615, 'learning_rate': 4.968192170184613e-05, 'epoch': 0.02}\n",
            "  1% 500/78597 [01:02<2:35:06,  8.39it/s][INFO|trainer.py:1908] 2021-07-04 21:35:43,099 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:35:43,100 >> Configuration saved in /tmp/tst-summarization/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:35:44,767 >> Model weights saved in /tmp/tst-summarization/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:35:44,767 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:35:44,767 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.7672, 'learning_rate': 4.936384340369226e-05, 'epoch': 0.04}\n",
            "  1% 1000/78597 [02:15<2:53:57,  7.43it/s][INFO|trainer.py:1908] 2021-07-04 21:36:55,857 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-1000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:36:55,858 >> Configuration saved in /tmp/tst-summarization/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:36:57,455 >> Model weights saved in /tmp/tst-summarization/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:36:57,456 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:36:57,456 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.736, 'learning_rate': 4.904576510553838e-05, 'epoch': 0.06}\n",
            "  2% 1500/78597 [03:30<2:42:58,  7.88it/s][INFO|trainer.py:1908] 2021-07-04 21:38:10,688 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-1500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:38:10,689 >> Configuration saved in /tmp/tst-summarization/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:38:12,228 >> Model weights saved in /tmp/tst-summarization/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:38:12,229 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:38:12,229 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.7163, 'learning_rate': 4.872768680738451e-05, 'epoch': 0.08}\n",
            "  3% 2000/78597 [04:41<2:26:50,  8.69it/s][INFO|trainer.py:1908] 2021-07-04 21:39:22,176 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-2000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:39:22,177 >> Configuration saved in /tmp/tst-summarization/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:39:23,709 >> Model weights saved in /tmp/tst-summarization/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:39:23,710 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:39:23,710 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.6767, 'learning_rate': 4.840960850923064e-05, 'epoch': 0.1}\n",
            "  3% 2500/78597 [05:55<2:46:39,  7.61it/s][INFO|trainer.py:1908] 2021-07-04 21:40:35,870 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-2500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:40:35,871 >> Configuration saved in /tmp/tst-summarization/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:40:37,423 >> Model weights saved in /tmp/tst-summarization/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:40:37,424 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:40:37,424 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.6772, 'learning_rate': 4.809153021107676e-05, 'epoch': 0.11}\n",
            "  4% 3000/78597 [07:15<2:45:47,  7.60it/s][INFO|trainer.py:1908] 2021-07-04 21:41:55,970 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-3000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:41:55,971 >> Configuration saved in /tmp/tst-summarization/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:41:57,882 >> Model weights saved in /tmp/tst-summarization/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:41:57,882 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:41:57,882 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:42:05,814 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.6921, 'learning_rate': 4.7773451912922887e-05, 'epoch': 0.13}\n",
            "  4% 3500/78597 [08:34<2:19:23,  8.98it/s][INFO|trainer.py:1908] 2021-07-04 21:43:15,190 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-3500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:43:15,191 >> Configuration saved in /tmp/tst-summarization/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:43:16,836 >> Model weights saved in /tmp/tst-summarization/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:43:16,837 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:43:16,837 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:43:24,799 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.6584, 'learning_rate': 4.745537361476902e-05, 'epoch': 0.15}\n",
            "  5% 4000/78597 [09:51<2:31:51,  8.19it/s][INFO|trainer.py:1908] 2021-07-04 21:44:31,784 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-4000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:44:31,785 >> Configuration saved in /tmp/tst-summarization/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:44:33,483 >> Model weights saved in /tmp/tst-summarization/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:44:33,483 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:44:33,483 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:44:41,676 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 0.6242, 'learning_rate': 4.7137295316615135e-05, 'epoch': 0.17}\n",
            "  6% 4500/78597 [11:09<2:35:57,  7.92it/s][INFO|trainer.py:1908] 2021-07-04 21:45:49,311 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-4500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:45:49,312 >> Configuration saved in /tmp/tst-summarization/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:45:51,045 >> Model weights saved in /tmp/tst-summarization/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:45:51,046 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:45:51,046 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:46:00,426 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 0.5958, 'learning_rate': 4.6819217018461266e-05, 'epoch': 0.19}\n",
            "  6% 5000/78597 [12:24<2:24:55,  8.46it/s][INFO|trainer.py:1908] 2021-07-04 21:47:04,509 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-5000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:47:04,510 >> Configuration saved in /tmp/tst-summarization/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:47:06,176 >> Model weights saved in /tmp/tst-summarization/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:47:06,177 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:47:06,177 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:47:15,204 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-2500] due to args.save_total_limit\n",
            "{'loss': 0.5863, 'learning_rate': 4.650113872030739e-05, 'epoch': 0.21}\n",
            "  7% 5500/78597 [13:43<2:47:27,  7.27it/s][INFO|trainer.py:1908] 2021-07-04 21:48:23,537 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-5500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:48:23,538 >> Configuration saved in /tmp/tst-summarization/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:48:25,219 >> Model weights saved in /tmp/tst-summarization/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:48:25,220 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:48:25,220 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:48:34,271 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-3000] due to args.save_total_limit\n",
            "{'loss': 0.5947, 'learning_rate': 4.6183060422153515e-05, 'epoch': 0.23}\n",
            "  8% 6000/78597 [15:01<2:22:16,  8.50it/s][INFO|trainer.py:1908] 2021-07-04 21:49:41,837 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-6000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:49:41,838 >> Configuration saved in /tmp/tst-summarization/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:49:43,711 >> Model weights saved in /tmp/tst-summarization/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:49:43,712 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:49:43,712 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-6000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:49:51,791 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-3500] due to args.save_total_limit\n",
            "{'loss': 0.5945, 'learning_rate': 4.5864982123999646e-05, 'epoch': 0.25}\n",
            "  8% 6500/78597 [16:17<2:41:34,  7.44it/s][INFO|trainer.py:1908] 2021-07-04 21:50:58,069 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-6500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:50:58,070 >> Configuration saved in /tmp/tst-summarization/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:50:59,885 >> Model weights saved in /tmp/tst-summarization/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:50:59,885 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:50:59,886 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-6500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:51:07,685 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-4000] due to args.save_total_limit\n",
            "{'loss': 0.5939, 'learning_rate': 4.554690382584577e-05, 'epoch': 0.27}\n",
            "  9% 7000/78597 [17:36<2:50:36,  6.99it/s][INFO|trainer.py:1908] 2021-07-04 21:52:16,677 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-7000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:52:16,678 >> Configuration saved in /tmp/tst-summarization/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:52:18,457 >> Model weights saved in /tmp/tst-summarization/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:52:18,457 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:52:18,458 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-7000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:52:29,350 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-4500] due to args.save_total_limit\n",
            "{'loss': 0.6101, 'learning_rate': 4.5228825527691895e-05, 'epoch': 0.29}\n",
            " 10% 7500/78597 [18:53<2:44:36,  7.20it/s][INFO|trainer.py:1908] 2021-07-04 21:53:33,386 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-7500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:53:33,387 >> Configuration saved in /tmp/tst-summarization/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:53:35,253 >> Model weights saved in /tmp/tst-summarization/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:53:35,254 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:53:35,254 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-7500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:53:44,230 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-5000] due to args.save_total_limit\n",
            "{'loss': 0.5161, 'learning_rate': 4.4910747229538026e-05, 'epoch': 0.31}\n",
            " 10% 8000/78597 [20:10<2:20:43,  8.36it/s][INFO|trainer.py:1908] 2021-07-04 21:54:50,292 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-8000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:54:50,293 >> Configuration saved in /tmp/tst-summarization/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:54:52,177 >> Model weights saved in /tmp/tst-summarization/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:54:52,177 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:54:52,178 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-8000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:55:01,891 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-5500] due to args.save_total_limit\n",
            "{'loss': 0.5674, 'learning_rate': 4.459266893138415e-05, 'epoch': 0.32}\n",
            " 11% 8500/78597 [21:27<2:42:57,  7.17it/s][INFO|trainer.py:1908] 2021-07-04 21:56:07,552 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-8500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:56:07,553 >> Configuration saved in /tmp/tst-summarization/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:56:09,278 >> Model weights saved in /tmp/tst-summarization/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:56:09,279 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:56:09,279 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-8500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:56:17,401 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-6000] due to args.save_total_limit\n",
            "{'loss': 0.5672, 'learning_rate': 4.427459063323028e-05, 'epoch': 0.34}\n",
            " 11% 9000/78597 [22:44<3:08:08,  6.17it/s][INFO|trainer.py:1908] 2021-07-04 21:57:24,652 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-9000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:57:24,652 >> Configuration saved in /tmp/tst-summarization/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:57:26,489 >> Model weights saved in /tmp/tst-summarization/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:57:26,489 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:57:26,490 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-9000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:57:34,637 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-6500] due to args.save_total_limit\n",
            "{'loss': 0.5068, 'learning_rate': 4.3956512335076406e-05, 'epoch': 0.36}\n",
            " 12% 9500/78597 [24:03<2:40:08,  7.19it/s][INFO|trainer.py:1908] 2021-07-04 21:58:43,794 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-9500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 21:58:43,795 >> Configuration saved in /tmp/tst-summarization/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 21:58:45,508 >> Model weights saved in /tmp/tst-summarization/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 21:58:45,508 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 21:58:45,508 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-9500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 21:58:53,491 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-7000] due to args.save_total_limit\n",
            "{'loss': 0.5351, 'learning_rate': 4.363843403692253e-05, 'epoch': 0.38}\n",
            " 13% 10000/78597 [25:21<2:10:22,  8.77it/s][INFO|trainer.py:1908] 2021-07-04 22:00:01,438 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-10000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:00:01,439 >> Configuration saved in /tmp/tst-summarization/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:00:03,154 >> Model weights saved in /tmp/tst-summarization/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:00:03,155 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:00:03,155 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-10000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:00:11,361 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-7500] due to args.save_total_limit\n",
            "{'loss': 0.535, 'learning_rate': 4.332035573876866e-05, 'epoch': 0.4}\n",
            " 13% 10500/78597 [26:36<2:07:54,  8.87it/s][INFO|trainer.py:1908] 2021-07-04 22:01:16,376 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-10500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:01:16,377 >> Configuration saved in /tmp/tst-summarization/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:01:18,290 >> Model weights saved in /tmp/tst-summarization/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:01:18,290 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:01:18,291 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-10500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:01:26,373 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-8000] due to args.save_total_limit\n",
            "{'loss': 0.5315, 'learning_rate': 4.3002277440614786e-05, 'epoch': 0.42}\n",
            " 14% 11000/78597 [27:53<2:39:22,  7.07it/s][INFO|trainer.py:1908] 2021-07-04 22:02:33,488 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-11000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:02:33,489 >> Configuration saved in /tmp/tst-summarization/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:02:35,192 >> Model weights saved in /tmp/tst-summarization/checkpoint-11000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:02:35,192 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-11000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:02:35,193 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-11000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:02:43,231 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-8500] due to args.save_total_limit\n",
            "{'loss': 0.472, 'learning_rate': 4.268419914246091e-05, 'epoch': 0.44}\n",
            " 15% 11500/78597 [29:09<3:16:17,  5.70it/s][INFO|trainer.py:1908] 2021-07-04 22:03:49,847 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-11500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:03:49,847 >> Configuration saved in /tmp/tst-summarization/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:03:51,663 >> Model weights saved in /tmp/tst-summarization/checkpoint-11500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:03:51,664 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-11500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:03:51,664 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-11500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:03:59,763 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-9000] due to args.save_total_limit\n",
            "{'loss': 0.4928, 'learning_rate': 4.236612084430704e-05, 'epoch': 0.46}\n",
            " 15% 12000/78597 [30:28<2:13:17,  8.33it/s][INFO|trainer.py:1908] 2021-07-04 22:05:08,320 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-12000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:05:08,321 >> Configuration saved in /tmp/tst-summarization/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:05:10,246 >> Model weights saved in /tmp/tst-summarization/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:05:10,247 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-12000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:05:10,247 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-12000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:05:18,234 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-9500] due to args.save_total_limit\n",
            "{'loss': 0.4876, 'learning_rate': 4.2048042546153166e-05, 'epoch': 0.48}\n",
            " 16% 12500/78597 [31:44<2:15:40,  8.12it/s][INFO|trainer.py:1908] 2021-07-04 22:06:24,321 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-12500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:06:24,321 >> Configuration saved in /tmp/tst-summarization/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:06:26,129 >> Model weights saved in /tmp/tst-summarization/checkpoint-12500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:06:26,129 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-12500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:06:26,130 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-12500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:06:35,078 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 0.4657, 'learning_rate': 4.1729964247999284e-05, 'epoch': 0.5}\n",
            " 17% 13000/78597 [32:59<2:00:38,  9.06it/s][INFO|trainer.py:1908] 2021-07-04 22:07:39,530 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-13000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:07:39,531 >> Configuration saved in /tmp/tst-summarization/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:07:41,274 >> Model weights saved in /tmp/tst-summarization/checkpoint-13000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:07:41,275 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-13000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:07:41,275 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-13000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:07:49,342 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-10500] due to args.save_total_limit\n",
            "{'loss': 0.516, 'learning_rate': 4.1411885949845415e-05, 'epoch': 0.52}\n",
            " 17% 13500/78597 [34:15<1:59:49,  9.05it/s][INFO|trainer.py:1908] 2021-07-04 22:08:55,283 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-13500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:08:55,284 >> Configuration saved in /tmp/tst-summarization/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:08:57,292 >> Model weights saved in /tmp/tst-summarization/checkpoint-13500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:08:57,292 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-13500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:08:57,293 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-13500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:09:05,043 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-11000] due to args.save_total_limit\n",
            "{'loss': 0.4752, 'learning_rate': 4.109380765169154e-05, 'epoch': 0.53}\n",
            " 18% 14000/78597 [35:33<3:02:29,  5.90it/s][INFO|trainer.py:1908] 2021-07-04 22:10:13,696 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-14000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:10:13,697 >> Configuration saved in /tmp/tst-summarization/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:10:15,556 >> Model weights saved in /tmp/tst-summarization/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:10:15,557 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-14000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:10:15,557 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-14000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:10:23,447 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-11500] due to args.save_total_limit\n",
            "{'loss': 0.4711, 'learning_rate': 4.077572935353767e-05, 'epoch': 0.55}\n",
            " 18% 14500/78597 [36:46<2:08:16,  8.33it/s][INFO|trainer.py:1908] 2021-07-04 22:11:26,495 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-14500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:11:26,495 >> Configuration saved in /tmp/tst-summarization/checkpoint-14500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:11:28,297 >> Model weights saved in /tmp/tst-summarization/checkpoint-14500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:11:28,298 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-14500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:11:28,298 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-14500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:11:36,190 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-12000] due to args.save_total_limit\n",
            "{'loss': 0.4607, 'learning_rate': 4.0457651055383795e-05, 'epoch': 0.57}\n",
            " 19% 15000/78597 [38:00<2:09:59,  8.15it/s][INFO|trainer.py:1908] 2021-07-04 22:12:41,137 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-15000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:12:41,138 >> Configuration saved in /tmp/tst-summarization/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:12:42,947 >> Model weights saved in /tmp/tst-summarization/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:12:42,948 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:12:42,948 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-15000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:12:50,910 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-12500] due to args.save_total_limit\n",
            "{'loss': 0.4503, 'learning_rate': 4.013957275722992e-05, 'epoch': 0.59}\n",
            " 20% 15500/78597 [39:16<1:58:39,  8.86it/s][INFO|trainer.py:1908] 2021-07-04 22:13:56,306 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-15500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:13:56,307 >> Configuration saved in /tmp/tst-summarization/checkpoint-15500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:13:58,052 >> Model weights saved in /tmp/tst-summarization/checkpoint-15500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:13:58,052 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-15500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:13:58,053 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-15500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:14:05,963 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-13000] due to args.save_total_limit\n",
            "{'loss': 0.4914, 'learning_rate': 3.982149445907605e-05, 'epoch': 0.61}\n",
            " 20% 16000/78597 [40:30<2:26:26,  7.12it/s][INFO|trainer.py:1908] 2021-07-04 22:15:10,946 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-16000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:15:10,946 >> Configuration saved in /tmp/tst-summarization/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:15:12,754 >> Model weights saved in /tmp/tst-summarization/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:15:12,754 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:15:12,755 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-16000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:15:21,018 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-13500] due to args.save_total_limit\n",
            "{'loss': 0.4524, 'learning_rate': 3.9503416160922175e-05, 'epoch': 0.63}\n",
            " 21% 16500/78597 [41:44<2:11:03,  7.90it/s][INFO|trainer.py:1908] 2021-07-04 22:16:24,680 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-16500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:16:24,681 >> Configuration saved in /tmp/tst-summarization/checkpoint-16500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:16:26,532 >> Model weights saved in /tmp/tst-summarization/checkpoint-16500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:16:26,532 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-16500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:16:26,532 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-16500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:16:34,434 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-14000] due to args.save_total_limit\n",
            "{'loss': 0.4464, 'learning_rate': 3.91853378627683e-05, 'epoch': 0.65}\n",
            " 22% 17000/78597 [43:00<1:57:04,  8.77it/s][INFO|trainer.py:1908] 2021-07-04 22:17:40,558 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-17000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:17:40,559 >> Configuration saved in /tmp/tst-summarization/checkpoint-17000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:17:42,207 >> Model weights saved in /tmp/tst-summarization/checkpoint-17000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:17:42,208 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-17000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:17:42,208 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-17000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:17:50,140 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-14500] due to args.save_total_limit\n",
            "{'loss': 0.4437, 'learning_rate': 3.886725956461443e-05, 'epoch': 0.67}\n",
            " 22% 17500/78597 [44:14<2:12:07,  7.71it/s][INFO|trainer.py:1908] 2021-07-04 22:18:54,643 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-17500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:18:54,644 >> Configuration saved in /tmp/tst-summarization/checkpoint-17500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:18:56,445 >> Model weights saved in /tmp/tst-summarization/checkpoint-17500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:18:56,446 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-17500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:18:56,446 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-17500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:19:04,545 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-15000] due to args.save_total_limit\n",
            "{'loss': 0.4197, 'learning_rate': 3.8549181266460554e-05, 'epoch': 0.69}\n",
            " 23% 18000/78597 [45:28<1:53:33,  8.89it/s][INFO|trainer.py:1908] 2021-07-04 22:20:08,955 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-18000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:20:08,956 >> Configuration saved in /tmp/tst-summarization/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:20:10,710 >> Model weights saved in /tmp/tst-summarization/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:20:10,710 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-18000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:20:10,711 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-18000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:20:18,753 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-15500] due to args.save_total_limit\n",
            "{'loss': 0.4804, 'learning_rate': 3.823110296830668e-05, 'epoch': 0.71}\n",
            " 24% 18500/78597 [46:44<2:24:28,  6.93it/s][INFO|trainer.py:1908] 2021-07-04 22:21:25,036 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-18500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:21:25,037 >> Configuration saved in /tmp/tst-summarization/checkpoint-18500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:21:26,943 >> Model weights saved in /tmp/tst-summarization/checkpoint-18500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:21:26,943 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-18500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:21:26,944 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-18500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:21:34,815 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-16000] due to args.save_total_limit\n",
            "{'loss': 0.4169, 'learning_rate': 3.791302467015281e-05, 'epoch': 0.73}\n",
            " 24% 19000/78597 [47:59<1:53:40,  8.74it/s][INFO|trainer.py:1908] 2021-07-04 22:22:39,691 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-19000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:22:39,692 >> Configuration saved in /tmp/tst-summarization/checkpoint-19000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:22:41,391 >> Model weights saved in /tmp/tst-summarization/checkpoint-19000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:22:41,392 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-19000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:22:41,392 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-19000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:22:49,348 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-16500] due to args.save_total_limit\n",
            "{'loss': 0.423, 'learning_rate': 3.7594946371998934e-05, 'epoch': 0.74}\n",
            " 25% 19500/78597 [49:15<1:56:50,  8.43it/s][INFO|trainer.py:1908] 2021-07-04 22:23:56,004 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-19500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:23:56,005 >> Configuration saved in /tmp/tst-summarization/checkpoint-19500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:23:57,818 >> Model weights saved in /tmp/tst-summarization/checkpoint-19500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:23:57,819 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-19500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:23:57,819 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-19500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:24:05,835 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-17000] due to args.save_total_limit\n",
            "{'loss': 0.4604, 'learning_rate': 3.727686807384506e-05, 'epoch': 0.76}\n",
            " 25% 20000/78597 [50:33<1:53:36,  8.60it/s][INFO|trainer.py:1908] 2021-07-04 22:25:13,423 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-20000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:25:13,424 >> Configuration saved in /tmp/tst-summarization/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:25:15,190 >> Model weights saved in /tmp/tst-summarization/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:25:15,191 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:25:15,191 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:25:23,214 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-17500] due to args.save_total_limit\n",
            "{'loss': 0.4143, 'learning_rate': 3.695878977569119e-05, 'epoch': 0.78}\n",
            " 26% 20500/78597 [51:48<1:50:50,  8.74it/s][INFO|trainer.py:1908] 2021-07-04 22:26:28,338 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-20500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:26:28,339 >> Configuration saved in /tmp/tst-summarization/checkpoint-20500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:26:30,127 >> Model weights saved in /tmp/tst-summarization/checkpoint-20500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:26:30,127 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-20500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:26:30,128 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-20500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:26:38,024 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-18000] due to args.save_total_limit\n",
            "{'loss': 0.4617, 'learning_rate': 3.6640711477537314e-05, 'epoch': 0.8}\n",
            " 27% 21000/78597 [53:03<1:46:18,  9.03it/s][INFO|trainer.py:1908] 2021-07-04 22:27:43,600 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-21000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:27:43,601 >> Configuration saved in /tmp/tst-summarization/checkpoint-21000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:27:45,441 >> Model weights saved in /tmp/tst-summarization/checkpoint-21000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:27:45,441 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-21000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:27:45,441 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-21000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:27:54,472 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-18500] due to args.save_total_limit\n",
            "{'loss': 0.4256, 'learning_rate': 3.632263317938344e-05, 'epoch': 0.82}\n",
            " 27% 21500/78597 [54:19<1:49:22,  8.70it/s][INFO|trainer.py:1908] 2021-07-04 22:28:59,742 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-21500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:28:59,743 >> Configuration saved in /tmp/tst-summarization/checkpoint-21500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:29:01,540 >> Model weights saved in /tmp/tst-summarization/checkpoint-21500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:29:01,541 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-21500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:29:01,541 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-21500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:29:09,393 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-19000] due to args.save_total_limit\n",
            "{'loss': 0.4205, 'learning_rate': 3.600455488122956e-05, 'epoch': 0.84}\n",
            " 28% 22000/78597 [55:34<1:49:59,  8.58it/s][INFO|trainer.py:1908] 2021-07-04 22:30:14,722 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-22000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:30:14,723 >> Configuration saved in /tmp/tst-summarization/checkpoint-22000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:30:16,482 >> Model weights saved in /tmp/tst-summarization/checkpoint-22000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:30:16,483 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-22000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:30:16,483 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-22000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:30:24,436 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-19500] due to args.save_total_limit\n",
            "{'loss': 0.4365, 'learning_rate': 3.568647658307569e-05, 'epoch': 0.86}\n",
            " 29% 22500/78597 [56:47<1:49:07,  8.57it/s][INFO|trainer.py:1908] 2021-07-04 22:31:28,153 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-22500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:31:28,154 >> Configuration saved in /tmp/tst-summarization/checkpoint-22500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:31:30,088 >> Model weights saved in /tmp/tst-summarization/checkpoint-22500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:31:30,089 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-22500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:31:30,089 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-22500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:31:38,083 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-20000] due to args.save_total_limit\n",
            "{'loss': 0.4485, 'learning_rate': 3.536839828492182e-05, 'epoch': 0.88}\n",
            " 29% 23000/78597 [58:05<1:49:02,  8.50it/s][INFO|trainer.py:1908] 2021-07-04 22:32:45,705 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-23000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:32:45,706 >> Configuration saved in /tmp/tst-summarization/checkpoint-23000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:32:47,623 >> Model weights saved in /tmp/tst-summarization/checkpoint-23000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:32:47,624 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-23000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:32:47,624 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-23000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:32:55,402 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-20500] due to args.save_total_limit\n",
            "{'loss': 0.3813, 'learning_rate': 3.505031998676794e-05, 'epoch': 0.9}\n",
            " 30% 23500/78597 [59:23<1:52:01,  8.20it/s][INFO|trainer.py:1908] 2021-07-04 22:34:03,412 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-23500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:34:03,413 >> Configuration saved in /tmp/tst-summarization/checkpoint-23500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:34:05,317 >> Model weights saved in /tmp/tst-summarization/checkpoint-23500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:34:05,317 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-23500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:34:05,317 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-23500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:34:13,024 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-21000] due to args.save_total_limit\n",
            "{'loss': 0.3473, 'learning_rate': 3.473224168861407e-05, 'epoch': 0.92}\n",
            " 31% 24000/78597 [1:00:38<1:50:32,  8.23it/s][INFO|trainer.py:1908] 2021-07-04 22:35:18,336 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-24000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:35:18,337 >> Configuration saved in /tmp/tst-summarization/checkpoint-24000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:35:20,307 >> Model weights saved in /tmp/tst-summarization/checkpoint-24000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:35:20,308 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-24000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:35:20,308 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-24000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:35:29,784 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-21500] due to args.save_total_limit\n",
            "{'loss': 0.4003, 'learning_rate': 3.44141633904602e-05, 'epoch': 0.94}\n",
            " 31% 24500/78597 [1:01:52<1:58:21,  7.62it/s][INFO|trainer.py:1908] 2021-07-04 22:36:33,037 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-24500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:36:33,037 >> Configuration saved in /tmp/tst-summarization/checkpoint-24500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:36:34,816 >> Model weights saved in /tmp/tst-summarization/checkpoint-24500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:36:34,817 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-24500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:36:34,817 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-24500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:36:42,644 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-22000] due to args.save_total_limit\n",
            "{'loss': 0.3593, 'learning_rate': 3.409608509230632e-05, 'epoch': 0.95}\n",
            " 32% 25000/78597 [1:03:04<1:43:49,  8.60it/s][INFO|trainer.py:1908] 2021-07-04 22:37:45,108 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-25000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:37:45,109 >> Configuration saved in /tmp/tst-summarization/checkpoint-25000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:37:47,027 >> Model weights saved in /tmp/tst-summarization/checkpoint-25000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:37:47,028 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-25000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:37:47,028 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-25000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:37:54,897 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-22500] due to args.save_total_limit\n",
            "{'loss': 0.4186, 'learning_rate': 3.377800679415245e-05, 'epoch': 0.97}\n",
            " 32% 25500/78597 [1:04:19<1:38:31,  8.98it/s][INFO|trainer.py:1908] 2021-07-04 22:38:59,857 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-25500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:38:59,857 >> Configuration saved in /tmp/tst-summarization/checkpoint-25500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:39:01,813 >> Model weights saved in /tmp/tst-summarization/checkpoint-25500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:39:01,813 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-25500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:39:01,814 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-25500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:39:09,721 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-23000] due to args.save_total_limit\n",
            "{'loss': 0.4083, 'learning_rate': 3.345992849599858e-05, 'epoch': 0.99}\n",
            " 33% 26000/78597 [1:05:36<2:06:50,  6.91it/s][INFO|trainer.py:1908] 2021-07-04 22:40:16,361 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-26000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:40:16,362 >> Configuration saved in /tmp/tst-summarization/checkpoint-26000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:40:18,189 >> Model weights saved in /tmp/tst-summarization/checkpoint-26000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:40:18,189 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-26000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:40:18,190 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-26000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:40:26,216 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-23500] due to args.save_total_limit\n",
            "{'loss': 0.284, 'learning_rate': 3.31418501978447e-05, 'epoch': 1.01}\n",
            " 34% 26500/78597 [1:06:53<2:08:15,  6.77it/s][INFO|trainer.py:1908] 2021-07-04 22:41:34,145 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-26500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:41:34,146 >> Configuration saved in /tmp/tst-summarization/checkpoint-26500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:41:35,811 >> Model weights saved in /tmp/tst-summarization/checkpoint-26500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:41:35,811 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-26500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:41:35,812 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-26500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:41:44,946 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-24000] due to args.save_total_limit\n",
            "{'loss': 0.2369, 'learning_rate': 3.2823771899690834e-05, 'epoch': 1.03}\n",
            " 34% 27000/78597 [1:08:07<1:38:28,  8.73it/s][INFO|trainer.py:1908] 2021-07-04 22:42:48,149 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-27000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:42:48,150 >> Configuration saved in /tmp/tst-summarization/checkpoint-27000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:42:50,167 >> Model weights saved in /tmp/tst-summarization/checkpoint-27000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:42:50,168 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-27000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:42:50,168 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-27000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:42:59,480 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-24500] due to args.save_total_limit\n",
            "{'loss': 0.2601, 'learning_rate': 3.250569360153696e-05, 'epoch': 1.05}\n",
            " 35% 27500/78597 [1:09:25<1:38:45,  8.62it/s][INFO|trainer.py:1908] 2021-07-04 22:44:05,254 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-27500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:44:05,255 >> Configuration saved in /tmp/tst-summarization/checkpoint-27500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:44:07,100 >> Model weights saved in /tmp/tst-summarization/checkpoint-27500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:44:07,100 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-27500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:44:07,101 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-27500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:44:14,889 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-25000] due to args.save_total_limit\n",
            "{'loss': 0.2274, 'learning_rate': 3.218761530338308e-05, 'epoch': 1.07}\n",
            " 36% 28000/78597 [1:10:37<1:31:15,  9.24it/s][INFO|trainer.py:1908] 2021-07-04 22:45:17,318 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-28000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:45:17,318 >> Configuration saved in /tmp/tst-summarization/checkpoint-28000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:45:19,225 >> Model weights saved in /tmp/tst-summarization/checkpoint-28000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:45:19,226 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-28000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:45:19,226 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-28000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:45:26,887 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-25500] due to args.save_total_limit\n",
            "{'loss': 0.266, 'learning_rate': 3.1869537005229214e-05, 'epoch': 1.09}\n",
            " 36% 28500/78597 [1:11:51<1:33:32,  8.93it/s][INFO|trainer.py:1908] 2021-07-04 22:46:31,513 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-28500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:46:31,514 >> Configuration saved in /tmp/tst-summarization/checkpoint-28500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:46:33,620 >> Model weights saved in /tmp/tst-summarization/checkpoint-28500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:46:33,620 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-28500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:46:33,621 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-28500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:46:41,502 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-26000] due to args.save_total_limit\n",
            "{'loss': 0.2451, 'learning_rate': 3.155145870707534e-05, 'epoch': 1.11}\n",
            " 37% 29000/78597 [1:13:06<1:45:22,  7.85it/s][INFO|trainer.py:1908] 2021-07-04 22:47:46,532 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-29000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:47:46,534 >> Configuration saved in /tmp/tst-summarization/checkpoint-29000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:47:48,428 >> Model weights saved in /tmp/tst-summarization/checkpoint-29000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:47:48,428 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-29000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:47:48,428 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-29000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:47:56,100 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-26500] due to args.save_total_limit\n",
            "{'loss': 0.2189, 'learning_rate': 3.123338040892146e-05, 'epoch': 1.13}\n",
            " 38% 29500/78597 [1:14:17<1:41:30,  8.06it/s][INFO|trainer.py:1908] 2021-07-04 22:48:58,171 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-29500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:48:58,172 >> Configuration saved in /tmp/tst-summarization/checkpoint-29500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:49:00,072 >> Model weights saved in /tmp/tst-summarization/checkpoint-29500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:49:00,073 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-29500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:49:00,073 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-29500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:49:07,643 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-27000] due to args.save_total_limit\n",
            "{'loss': 0.2494, 'learning_rate': 3.091530211076759e-05, 'epoch': 1.15}\n",
            " 38% 30000/78597 [1:15:33<1:40:18,  8.07it/s][INFO|trainer.py:1908] 2021-07-04 22:50:13,845 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-30000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:50:13,846 >> Configuration saved in /tmp/tst-summarization/checkpoint-30000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:50:15,873 >> Model weights saved in /tmp/tst-summarization/checkpoint-30000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:50:15,873 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-30000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:50:15,874 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-30000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:50:23,622 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-27500] due to args.save_total_limit\n",
            "{'loss': 0.2098, 'learning_rate': 3.059722381261371e-05, 'epoch': 1.16}\n",
            " 39% 30500/78597 [1:16:49<1:32:55,  8.63it/s][INFO|trainer.py:1908] 2021-07-04 22:51:29,699 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-30500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:51:29,699 >> Configuration saved in /tmp/tst-summarization/checkpoint-30500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:51:31,484 >> Model weights saved in /tmp/tst-summarization/checkpoint-30500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:51:31,485 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-30500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:51:31,485 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-30500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:51:39,320 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-28000] due to args.save_total_limit\n",
            "{'loss': 0.2647, 'learning_rate': 3.027914551445984e-05, 'epoch': 1.18}\n",
            " 39% 31000/78597 [1:18:03<1:46:54,  7.42it/s][INFO|trainer.py:1908] 2021-07-04 22:52:43,703 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-31000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:52:43,704 >> Configuration saved in /tmp/tst-summarization/checkpoint-31000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:52:45,585 >> Model weights saved in /tmp/tst-summarization/checkpoint-31000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:52:45,585 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-31000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:52:45,586 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-31000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:52:53,532 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-28500] due to args.save_total_limit\n",
            "{'loss': 0.2439, 'learning_rate': 2.9961067216305967e-05, 'epoch': 1.2}\n",
            " 40% 31500/78597 [1:19:15<1:41:05,  7.76it/s][INFO|trainer.py:1908] 2021-07-04 22:53:55,632 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-31500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:53:55,633 >> Configuration saved in /tmp/tst-summarization/checkpoint-31500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:53:57,549 >> Model weights saved in /tmp/tst-summarization/checkpoint-31500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:53:57,550 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-31500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:53:57,550 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-31500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:54:06,419 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-29000] due to args.save_total_limit\n",
            "{'loss': 0.2184, 'learning_rate': 2.964298891815209e-05, 'epoch': 1.22}\n",
            " 41% 32000/78597 [1:20:33<1:45:45,  7.34it/s][INFO|trainer.py:1908] 2021-07-04 22:55:13,994 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-32000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:55:13,995 >> Configuration saved in /tmp/tst-summarization/checkpoint-32000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:55:15,865 >> Model weights saved in /tmp/tst-summarization/checkpoint-32000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:55:15,866 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-32000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:55:15,866 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-32000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:55:23,723 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-29500] due to args.save_total_limit\n",
            "{'loss': 0.2188, 'learning_rate': 2.932491061999822e-05, 'epoch': 1.24}\n",
            " 41% 32500/78597 [1:21:46<1:36:01,  8.00it/s][INFO|trainer.py:1908] 2021-07-04 22:56:26,287 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-32500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:56:26,288 >> Configuration saved in /tmp/tst-summarization/checkpoint-32500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:56:28,203 >> Model weights saved in /tmp/tst-summarization/checkpoint-32500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:56:28,203 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-32500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:56:28,203 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-32500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:56:37,017 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-30000] due to args.save_total_limit\n",
            "{'loss': 0.2371, 'learning_rate': 2.9006832321844347e-05, 'epoch': 1.26}\n",
            " 42% 33000/78597 [1:23:02<1:59:39,  6.35it/s][INFO|trainer.py:1908] 2021-07-04 22:57:42,430 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-33000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:57:42,431 >> Configuration saved in /tmp/tst-summarization/checkpoint-33000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:57:44,380 >> Model weights saved in /tmp/tst-summarization/checkpoint-33000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:57:44,381 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-33000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:57:44,381 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-33000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:57:52,222 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-30500] due to args.save_total_limit\n",
            "{'loss': 0.2359, 'learning_rate': 2.8688754023690474e-05, 'epoch': 1.28}\n",
            " 43% 33500/78597 [1:24:15<1:43:45,  7.24it/s][INFO|trainer.py:1908] 2021-07-04 22:58:55,787 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-33500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 22:58:55,787 >> Configuration saved in /tmp/tst-summarization/checkpoint-33500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 22:58:57,612 >> Model weights saved in /tmp/tst-summarization/checkpoint-33500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 22:58:57,613 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-33500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 22:58:57,613 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-33500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 22:59:05,362 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-31000] due to args.save_total_limit\n",
            "{'loss': 0.2056, 'learning_rate': 2.83706757255366e-05, 'epoch': 1.3}\n",
            " 43% 34000/78597 [1:25:29<1:34:35,  7.86it/s][INFO|trainer.py:1908] 2021-07-04 23:00:09,754 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-34000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:00:09,755 >> Configuration saved in /tmp/tst-summarization/checkpoint-34000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:00:11,760 >> Model weights saved in /tmp/tst-summarization/checkpoint-34000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:00:11,760 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-34000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:00:11,760 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-34000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:00:19,397 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-31500] due to args.save_total_limit\n",
            "{'loss': 0.2264, 'learning_rate': 2.8052597427382727e-05, 'epoch': 1.32}\n",
            " 44% 34500/78597 [1:26:42<1:26:20,  8.51it/s][INFO|trainer.py:1908] 2021-07-04 23:01:22,748 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-34500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:01:22,748 >> Configuration saved in /tmp/tst-summarization/checkpoint-34500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:01:24,836 >> Model weights saved in /tmp/tst-summarization/checkpoint-34500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:01:24,836 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-34500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:01:24,836 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-34500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:01:32,513 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-32000] due to args.save_total_limit\n",
            "{'loss': 0.2404, 'learning_rate': 2.7734519129228854e-05, 'epoch': 1.34}\n",
            " 45% 35000/78597 [1:27:59<1:27:29,  8.31it/s][INFO|trainer.py:1908] 2021-07-04 23:02:40,057 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-35000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:02:40,058 >> Configuration saved in /tmp/tst-summarization/checkpoint-35000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:02:41,986 >> Model weights saved in /tmp/tst-summarization/checkpoint-35000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:02:41,987 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-35000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:02:41,987 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-35000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:02:50,833 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-32500] due to args.save_total_limit\n",
            "{'loss': 0.1861, 'learning_rate': 2.741644083107498e-05, 'epoch': 1.36}\n",
            " 45% 35500/78597 [1:29:17<1:43:02,  6.97it/s][INFO|trainer.py:1908] 2021-07-04 23:03:57,821 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-35500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:03:57,822 >> Configuration saved in /tmp/tst-summarization/checkpoint-35500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:03:59,544 >> Model weights saved in /tmp/tst-summarization/checkpoint-35500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:03:59,545 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-35500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:03:59,545 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-35500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:04:07,678 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-33000] due to args.save_total_limit\n",
            "{'loss': 0.2597, 'learning_rate': 2.7098362532921106e-05, 'epoch': 1.37}\n",
            " 46% 36000/78597 [1:30:32<1:22:06,  8.65it/s][INFO|trainer.py:1908] 2021-07-04 23:05:12,957 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-36000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:05:12,958 >> Configuration saved in /tmp/tst-summarization/checkpoint-36000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:05:14,979 >> Model weights saved in /tmp/tst-summarization/checkpoint-36000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:05:14,980 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-36000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:05:14,980 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-36000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:05:22,648 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-33500] due to args.save_total_limit\n",
            "{'loss': 0.2083, 'learning_rate': 2.6780284234767234e-05, 'epoch': 1.39}\n",
            " 46% 36500/78597 [1:31:45<1:34:42,  7.41it/s][INFO|trainer.py:1908] 2021-07-04 23:06:26,240 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-36500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:06:26,241 >> Configuration saved in /tmp/tst-summarization/checkpoint-36500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:06:28,140 >> Model weights saved in /tmp/tst-summarization/checkpoint-36500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:06:28,140 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-36500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:06:28,141 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-36500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:06:36,230 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-34000] due to args.save_total_limit\n",
            "{'loss': 0.2048, 'learning_rate': 2.646220593661336e-05, 'epoch': 1.41}\n",
            " 47% 37000/78597 [1:33:01<1:23:04,  8.34it/s][INFO|trainer.py:1908] 2021-07-04 23:07:41,630 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-37000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:07:41,631 >> Configuration saved in /tmp/tst-summarization/checkpoint-37000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:07:43,568 >> Model weights saved in /tmp/tst-summarization/checkpoint-37000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:07:43,569 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-37000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:07:43,569 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-37000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:07:51,388 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-34500] due to args.save_total_limit\n",
            "{'loss': 0.2153, 'learning_rate': 2.6144127638459486e-05, 'epoch': 1.43}\n",
            " 48% 37500/78597 [1:34:14<1:36:14,  7.12it/s][INFO|trainer.py:1908] 2021-07-04 23:08:54,481 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-37500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:08:54,482 >> Configuration saved in /tmp/tst-summarization/checkpoint-37500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:08:56,494 >> Model weights saved in /tmp/tst-summarization/checkpoint-37500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:08:56,495 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-37500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:08:56,495 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-37500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:09:04,156 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-35000] due to args.save_total_limit\n",
            "{'loss': 0.2184, 'learning_rate': 2.5826049340305614e-05, 'epoch': 1.45}\n",
            " 48% 38000/78597 [1:35:27<1:23:13,  8.13it/s][INFO|trainer.py:1908] 2021-07-04 23:10:07,787 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-38000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:10:07,787 >> Configuration saved in /tmp/tst-summarization/checkpoint-38000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:10:09,669 >> Model weights saved in /tmp/tst-summarization/checkpoint-38000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:10:09,669 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-38000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:10:09,670 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-38000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:10:17,706 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-35500] due to args.save_total_limit\n",
            "{'loss': 0.2538, 'learning_rate': 2.5507971042151735e-05, 'epoch': 1.47}\n",
            " 49% 38500/78597 [1:36:44<1:22:05,  8.14it/s][INFO|trainer.py:1908] 2021-07-04 23:11:24,760 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-38500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:11:24,761 >> Configuration saved in /tmp/tst-summarization/checkpoint-38500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:11:26,700 >> Model weights saved in /tmp/tst-summarization/checkpoint-38500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:11:26,700 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-38500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:11:26,701 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-38500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:11:34,320 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-36000] due to args.save_total_limit\n",
            "{'loss': 0.2059, 'learning_rate': 2.5189892743997863e-05, 'epoch': 1.49}\n",
            " 50% 39000/78597 [1:37:58<1:23:35,  7.90it/s][INFO|trainer.py:1908] 2021-07-04 23:12:38,367 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-39000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:12:38,368 >> Configuration saved in /tmp/tst-summarization/checkpoint-39000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:12:40,342 >> Model weights saved in /tmp/tst-summarization/checkpoint-39000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:12:40,343 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-39000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:12:40,343 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-39000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:12:48,925 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-36500] due to args.save_total_limit\n",
            "{'loss': 0.2698, 'learning_rate': 2.487181444584399e-05, 'epoch': 1.51}\n",
            " 50% 39500/78597 [1:39:12<1:12:57,  8.93it/s][INFO|trainer.py:1908] 2021-07-04 23:13:52,949 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-39500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:13:52,950 >> Configuration saved in /tmp/tst-summarization/checkpoint-39500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:13:54,845 >> Model weights saved in /tmp/tst-summarization/checkpoint-39500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:13:54,846 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-39500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:13:54,846 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-39500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:14:02,484 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-37000] due to args.save_total_limit\n",
            "{'loss': 0.2478, 'learning_rate': 2.455373614769012e-05, 'epoch': 1.53}\n",
            " 51% 40000/78597 [1:40:28<1:17:48,  8.27it/s][INFO|trainer.py:1908] 2021-07-04 23:15:08,565 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-40000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:15:08,566 >> Configuration saved in /tmp/tst-summarization/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:15:10,480 >> Model weights saved in /tmp/tst-summarization/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:15:10,480 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:15:10,480 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-40000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:15:18,459 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-37500] due to args.save_total_limit\n",
            "{'loss': 0.2003, 'learning_rate': 2.4235657849536243e-05, 'epoch': 1.55}\n",
            " 52% 40500/78597 [1:41:42<1:16:39,  8.28it/s][INFO|trainer.py:1908] 2021-07-04 23:16:22,319 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-40500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:16:22,320 >> Configuration saved in /tmp/tst-summarization/checkpoint-40500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:16:24,335 >> Model weights saved in /tmp/tst-summarization/checkpoint-40500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:16:24,336 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-40500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:16:24,336 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-40500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:16:32,070 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-38000] due to args.save_total_limit\n",
            "{'loss': 0.2368, 'learning_rate': 2.3917579551382367e-05, 'epoch': 1.56}\n",
            " 52% 41000/78597 [1:42:55<1:22:20,  7.61it/s][INFO|trainer.py:1908] 2021-07-04 23:17:36,108 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-41000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:17:36,109 >> Configuration saved in /tmp/tst-summarization/checkpoint-41000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:17:38,022 >> Model weights saved in /tmp/tst-summarization/checkpoint-41000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:17:38,023 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-41000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:17:38,023 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-41000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:17:46,749 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-38500] due to args.save_total_limit\n",
            "{'loss': 0.217, 'learning_rate': 2.3599501253228495e-05, 'epoch': 1.58}\n",
            " 53% 41500/78597 [1:44:11<1:10:53,  8.72it/s][INFO|trainer.py:1908] 2021-07-04 23:18:51,531 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-41500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:18:51,532 >> Configuration saved in /tmp/tst-summarization/checkpoint-41500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:18:53,330 >> Model weights saved in /tmp/tst-summarization/checkpoint-41500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:18:53,331 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-41500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:18:53,331 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-41500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:19:01,530 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-39000] due to args.save_total_limit\n",
            "{'loss': 0.2393, 'learning_rate': 2.3281422955074623e-05, 'epoch': 1.6}\n",
            " 53% 42000/78597 [1:45:24<1:32:32,  6.59it/s][INFO|trainer.py:1908] 2021-07-04 23:20:04,642 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-42000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:20:04,643 >> Configuration saved in /tmp/tst-summarization/checkpoint-42000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:20:06,604 >> Model weights saved in /tmp/tst-summarization/checkpoint-42000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:20:06,604 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-42000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:20:06,604 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-42000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:20:14,290 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-39500] due to args.save_total_limit\n",
            "{'loss': 0.1865, 'learning_rate': 2.296334465692075e-05, 'epoch': 1.62}\n",
            " 54% 42500/78597 [1:46:38<1:12:00,  8.35it/s][INFO|trainer.py:1908] 2021-07-04 23:21:19,080 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-42500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:21:19,081 >> Configuration saved in /tmp/tst-summarization/checkpoint-42500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:21:20,948 >> Model weights saved in /tmp/tst-summarization/checkpoint-42500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:21:20,949 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-42500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:21:20,949 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-42500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:21:28,603 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-40000] due to args.save_total_limit\n",
            "{'loss': 0.212, 'learning_rate': 2.2645266358766875e-05, 'epoch': 1.64}\n",
            " 55% 43000/78597 [1:47:53<1:27:01,  6.82it/s][INFO|trainer.py:1908] 2021-07-04 23:22:33,541 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-43000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:22:33,542 >> Configuration saved in /tmp/tst-summarization/checkpoint-43000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:22:35,327 >> Model weights saved in /tmp/tst-summarization/checkpoint-43000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:22:35,328 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-43000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:22:35,328 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-43000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:22:43,175 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-40500] due to args.save_total_limit\n",
            "{'loss': 0.2009, 'learning_rate': 2.2327188060613003e-05, 'epoch': 1.66}\n",
            " 55% 43500/78597 [1:49:06<1:07:43,  8.64it/s][INFO|trainer.py:1908] 2021-07-04 23:23:47,051 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-43500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:23:47,052 >> Configuration saved in /tmp/tst-summarization/checkpoint-43500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:23:49,030 >> Model weights saved in /tmp/tst-summarization/checkpoint-43500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:23:49,031 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-43500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:23:49,031 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-43500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:23:56,968 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-41000] due to args.save_total_limit\n",
            "{'loss': 0.2061, 'learning_rate': 2.200910976245913e-05, 'epoch': 1.68}\n",
            " 56% 44000/78597 [1:50:21<1:22:51,  6.96it/s][INFO|trainer.py:1908] 2021-07-04 23:25:01,735 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-44000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:25:01,736 >> Configuration saved in /tmp/tst-summarization/checkpoint-44000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:25:03,628 >> Model weights saved in /tmp/tst-summarization/checkpoint-44000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:25:03,628 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-44000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:25:03,628 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-44000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:25:12,475 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-41500] due to args.save_total_limit\n",
            "{'loss': 0.2063, 'learning_rate': 2.1691031464305255e-05, 'epoch': 1.7}\n",
            " 57% 44500/78597 [1:51:36<1:07:29,  8.42it/s][INFO|trainer.py:1908] 2021-07-04 23:26:16,586 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-44500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:26:16,587 >> Configuration saved in /tmp/tst-summarization/checkpoint-44500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:26:18,493 >> Model weights saved in /tmp/tst-summarization/checkpoint-44500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:26:18,493 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-44500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:26:18,493 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-44500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:26:26,295 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-42000] due to args.save_total_limit\n",
            "{'loss': 0.1881, 'learning_rate': 2.137295316615138e-05, 'epoch': 1.72}\n",
            " 57% 45000/78597 [1:52:50<1:06:47,  8.38it/s][INFO|trainer.py:1908] 2021-07-04 23:27:30,289 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-45000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:27:30,290 >> Configuration saved in /tmp/tst-summarization/checkpoint-45000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:27:32,271 >> Model weights saved in /tmp/tst-summarization/checkpoint-45000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:27:32,272 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-45000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:27:32,272 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-45000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:27:39,967 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-42500] due to args.save_total_limit\n",
            "{'loss': 0.2177, 'learning_rate': 2.1054874867997507e-05, 'epoch': 1.74}\n",
            " 58% 45500/78597 [1:54:05<1:01:40,  8.94it/s][INFO|trainer.py:1908] 2021-07-04 23:28:46,224 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-45500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:28:46,225 >> Configuration saved in /tmp/tst-summarization/checkpoint-45500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:28:48,185 >> Model weights saved in /tmp/tst-summarization/checkpoint-45500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:28:48,185 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-45500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:28:48,185 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-45500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:28:55,786 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-43000] due to args.save_total_limit\n",
            "{'loss': 0.235, 'learning_rate': 2.0736796569843635e-05, 'epoch': 1.76}\n",
            " 59% 46000/78597 [1:55:18<1:07:09,  8.09it/s][INFO|trainer.py:1908] 2021-07-04 23:29:59,129 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-46000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:29:59,129 >> Configuration saved in /tmp/tst-summarization/checkpoint-46000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:30:00,815 >> Model weights saved in /tmp/tst-summarization/checkpoint-46000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:30:00,815 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-46000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:30:00,815 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-46000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:30:08,793 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-43500] due to args.save_total_limit\n",
            "{'loss': 0.1939, 'learning_rate': 2.041871827168976e-05, 'epoch': 1.77}\n",
            " 59% 46500/78597 [1:56:31<1:11:17,  7.50it/s][INFO|trainer.py:1908] 2021-07-04 23:31:12,203 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-46500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:31:12,204 >> Configuration saved in /tmp/tst-summarization/checkpoint-46500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:31:14,185 >> Model weights saved in /tmp/tst-summarization/checkpoint-46500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:31:14,185 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-46500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:31:14,186 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-46500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:31:21,912 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-44000] due to args.save_total_limit\n",
            "{'loss': 0.1761, 'learning_rate': 2.0100639973535887e-05, 'epoch': 1.79}\n",
            " 60% 47000/78597 [1:57:45<1:05:59,  7.98it/s][INFO|trainer.py:1908] 2021-07-04 23:32:26,043 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-47000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:32:26,044 >> Configuration saved in /tmp/tst-summarization/checkpoint-47000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:32:27,948 >> Model weights saved in /tmp/tst-summarization/checkpoint-47000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:32:27,949 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-47000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:32:27,949 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-47000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:32:35,610 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-44500] due to args.save_total_limit\n",
            "{'loss': 0.2042, 'learning_rate': 1.9782561675382015e-05, 'epoch': 1.81}\n",
            " 60% 47500/78597 [1:59:00<1:10:08,  7.39it/s][INFO|trainer.py:1908] 2021-07-04 23:33:40,909 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-47500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:33:40,909 >> Configuration saved in /tmp/tst-summarization/checkpoint-47500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:33:42,777 >> Model weights saved in /tmp/tst-summarization/checkpoint-47500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:33:42,778 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-47500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:33:42,778 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-47500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:33:50,622 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-45000] due to args.save_total_limit\n",
            "{'loss': 0.1818, 'learning_rate': 1.946448337722814e-05, 'epoch': 1.83}\n",
            " 61% 48000/78597 [2:00:16<57:49,  8.82it/s][INFO|trainer.py:1908] 2021-07-04 23:34:56,602 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-48000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:34:56,603 >> Configuration saved in /tmp/tst-summarization/checkpoint-48000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:34:58,603 >> Model weights saved in /tmp/tst-summarization/checkpoint-48000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:34:58,603 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-48000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:34:58,604 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-48000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:35:06,282 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-45500] due to args.save_total_limit\n",
            "{'loss': 0.1845, 'learning_rate': 1.9146405079074267e-05, 'epoch': 1.85}\n",
            " 62% 48500/78597 [2:01:30<1:03:52,  7.85it/s][INFO|trainer.py:1908] 2021-07-04 23:36:11,187 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-48500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:36:11,188 >> Configuration saved in /tmp/tst-summarization/checkpoint-48500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:36:13,038 >> Model weights saved in /tmp/tst-summarization/checkpoint-48500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:36:13,039 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-48500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:36:13,039 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-48500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:36:21,768 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-46000] due to args.save_total_limit\n",
            "{'loss': 0.1935, 'learning_rate': 1.882832678092039e-05, 'epoch': 1.87}\n",
            " 62% 49000/78597 [2:02:44<1:07:11,  7.34it/s][INFO|trainer.py:1908] 2021-07-04 23:37:24,547 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-49000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:37:24,548 >> Configuration saved in /tmp/tst-summarization/checkpoint-49000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:37:26,407 >> Model weights saved in /tmp/tst-summarization/checkpoint-49000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:37:26,408 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-49000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:37:26,408 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-49000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:37:34,122 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-46500] due to args.save_total_limit\n",
            "{'loss': 0.1573, 'learning_rate': 1.851024848276652e-05, 'epoch': 1.89}\n",
            " 63% 49500/78597 [2:03:55<1:04:21,  7.54it/s][INFO|trainer.py:1908] 2021-07-04 23:38:36,036 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-49500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:38:36,037 >> Configuration saved in /tmp/tst-summarization/checkpoint-49500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:38:38,061 >> Model weights saved in /tmp/tst-summarization/checkpoint-49500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:38:38,062 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-49500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:38:38,062 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-49500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:38:45,768 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-47000] due to args.save_total_limit\n",
            "{'loss': 0.2047, 'learning_rate': 1.8192170184612643e-05, 'epoch': 1.91}\n",
            " 64% 50000/78597 [2:05:09<59:58,  7.95it/s][INFO|trainer.py:1908] 2021-07-04 23:39:49,874 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-50000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:39:49,876 >> Configuration saved in /tmp/tst-summarization/checkpoint-50000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:39:51,778 >> Model weights saved in /tmp/tst-summarization/checkpoint-50000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:39:51,779 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-50000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:39:51,779 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-50000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:40:00,594 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-47500] due to args.save_total_limit\n",
            "{'loss': 0.1887, 'learning_rate': 1.787409188645877e-05, 'epoch': 1.93}\n",
            " 64% 50500/78597 [2:06:24<57:04,  8.21it/s][INFO|trainer.py:1908] 2021-07-04 23:41:04,483 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-50500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:41:04,484 >> Configuration saved in /tmp/tst-summarization/checkpoint-50500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:41:06,398 >> Model weights saved in /tmp/tst-summarization/checkpoint-50500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:41:06,398 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-50500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:41:06,399 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-50500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:41:14,279 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-48000] due to args.save_total_limit\n",
            "{'loss': 0.2136, 'learning_rate': 1.75560135883049e-05, 'epoch': 1.95}\n",
            " 65% 51000/78597 [2:07:37<56:32,  8.14it/s][INFO|trainer.py:1908] 2021-07-04 23:42:17,830 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-51000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:42:17,831 >> Configuration saved in /tmp/tst-summarization/checkpoint-51000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:42:19,880 >> Model weights saved in /tmp/tst-summarization/checkpoint-51000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:42:19,880 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-51000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:42:19,880 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-51000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:42:29,365 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-48500] due to args.save_total_limit\n",
            "{'loss': 0.2075, 'learning_rate': 1.7237935290151026e-05, 'epoch': 1.97}\n",
            " 66% 51500/78597 [2:08:54<56:07,  8.05it/s][INFO|trainer.py:1908] 2021-07-04 23:43:34,347 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-51500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:43:34,349 >> Configuration saved in /tmp/tst-summarization/checkpoint-51500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:43:36,234 >> Model weights saved in /tmp/tst-summarization/checkpoint-51500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:43:36,235 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-51500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:43:36,235 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-51500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:43:43,957 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-49000] due to args.save_total_limit\n",
            "{'loss': 0.1862, 'learning_rate': 1.691985699199715e-05, 'epoch': 1.98}\n",
            " 66% 52000/78597 [2:10:07<54:35,  8.12it/s][INFO|trainer.py:1908] 2021-07-04 23:44:47,450 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-52000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:44:47,451 >> Configuration saved in /tmp/tst-summarization/checkpoint-52000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:44:49,435 >> Model weights saved in /tmp/tst-summarization/checkpoint-52000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:44:49,436 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-52000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:44:49,436 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-52000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:44:57,195 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-49500] due to args.save_total_limit\n",
            "{'loss': 0.1364, 'learning_rate': 1.660177869384328e-05, 'epoch': 2.0}\n",
            " 67% 52500/78597 [2:11:19<59:48,  7.27it/s][INFO|trainer.py:1908] 2021-07-04 23:45:59,346 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-52500\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:45:59,346 >> Configuration saved in /tmp/tst-summarization/checkpoint-52500/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:46:01,496 >> Model weights saved in /tmp/tst-summarization/checkpoint-52500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:46:01,497 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-52500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:46:01,497 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-52500/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:46:10,106 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-50000] due to args.save_total_limit\n",
            "{'loss': 0.0804, 'learning_rate': 1.6283700395689403e-05, 'epoch': 2.02}\n",
            " 67% 53000/78597 [2:12:34<53:04,  8.04it/s][INFO|trainer.py:1908] 2021-07-04 23:47:14,494 >> Saving model checkpoint to /tmp/tst-summarization/checkpoint-53000\n",
            "[INFO|configuration_utils.py:371] 2021-07-04 23:47:14,495 >> Configuration saved in /tmp/tst-summarization/checkpoint-53000/config.json\n",
            "[INFO|modeling_utils.py:968] 2021-07-04 23:47:16,451 >> Model weights saved in /tmp/tst-summarization/checkpoint-53000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-04 23:47:16,452 >> tokenizer config file saved in /tmp/tst-summarization/checkpoint-53000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-04 23:47:16,452 >> Special tokens file saved in /tmp/tst-summarization/checkpoint-53000/special_tokens_map.json\n",
            "[INFO|trainer.py:1984] 2021-07-04 23:47:24,118 >> Deleting older checkpoint [/tmp/tst-summarization/checkpoint-50500] due to args.save_total_limit\n",
            " 68% 53341/78597 [2:13:27<51:19,  8.20it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKujmDObS_rO",
        "outputId": "418958a4-2d17-4cc6-ebfb-f6623cd69156"
      },
      "source": [
        "!zip -r bart.zip /tmp/tst-summarization/checkpoint-78500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tzip warning: name not matched: /dpr\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r dpr.zip . -i /dpr)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvljmjzESG2J"
      },
      "source": [
        "!pip install --upgrade gupload\n",
        "\n",
        "from google.colab import auth\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gupload --to '1W9R77oTk_DAMxfSjKGOLNj5xFtoC7gLF' bart.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGB9eUewTXYe",
        "outputId": "b4b8ba9b-0d4b-4245-f93e-9ca3e177203e"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}